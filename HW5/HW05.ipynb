{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "copyHW05_ZH_BIG.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6FxD5TJdPzSz",
        "pmLitTHjPzS0",
        "fyboqOiNPzS5",
        "Yfz1DvQnPzS7",
        "fdn-lLFuPzS9",
        "uxLBEPFzPzS9",
        "u4PoynUAPzS-",
        "eVvQOVA2PzS-",
        "1mCWLDf-PzS_",
        "bFIn7JiMPzS_",
        "frTt4nH8PzTA",
        "gINK2eCMPzTB",
        "B3hC9aODPzTB",
        "8eCELGqKPzTC",
        "tTerxx_2PzTD",
        "T-9XkN3bPzTD",
        "AJH_vv4WPzTE",
        "_O9kcP_DPzTF",
        "EfV5PgCTPzTG",
        "If54jC_CPzTG",
        "gUrTWHfDPzTG",
        "DPzaBnD0PzTH",
        "NGQV2TOtPzTH",
        "edM6C5JtPzTI",
        "UOI2K6_pPzTJ",
        "TNfFpKoFPzTP",
        "gciTr_qUPzTQ",
        "ipi0gIwsPzTQ",
        "73QNgfxEPzTQ",
        "TLTcbPWOPzTR",
        "PdKl5ZO-PzTR",
        "HeczZN_4PzTR",
        "K5nBt8pqPzTS",
        "suE-NQ_iPzTS",
        "chHWkbZ7PzTS",
        "sImfdzBHPzTT",
        "juazXP2SPzTT"
      ]
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2367de31cd134581b1d69be9ff9c5777": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1e9a9edd72a041758d24656fb3eed7cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_494b6e25805a43938dd5885b9ccd3f04",
              "IPY_MODEL_d54a4fb73d984c6f80565dbc2052c11e"
            ]
          }
        },
        "1e9a9edd72a041758d24656fb3eed7cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "494b6e25805a43938dd5885b9ccd3f04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_970ec52792784da9b97d54b5fef1ec04",
            "_dom_classes": [],
            "description": "train epoch 1:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 3790,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 18,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe44711eae42413dabf6aae3550d399a"
          }
        },
        "d54a4fb73d984c6f80565dbc2052c11e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5266710090474663bc876d53b07da34b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 18/3790 [00:16&lt;53:44,  1.17it/s, loss=2.17]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0deda2826060469ba6e073b5d62e041a"
          }
        },
        "970ec52792784da9b97d54b5fef1ec04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe44711eae42413dabf6aae3550d399a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5266710090474663bc876d53b07da34b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0deda2826060469ba6e073b5d62e041a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e32030000c8c499e8708ea71b217ae73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_86708699449b492ba06ca666c1321659",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4d4bf0bbf5ec498cafa273dcd9c7dc8c",
              "IPY_MODEL_b7e2216fa89c4f57939b4659343881d5"
            ]
          }
        },
        "86708699449b492ba06ca666c1321659": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d4bf0bbf5ec498cafa273dcd9c7dc8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a931c1e81c8349639bd08835398e115f",
            "_dom_classes": [],
            "description": "validation: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 41,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 41,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f564460097d44665bb83fc516c3fb8bf"
          }
        },
        "b7e2216fa89c4f57939b4659343881d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8169dc77028945cebe5f0fccb53ec580",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 41/41 [01:42&lt;00:00,  3.26s/it, valid_loss=3.03]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_42fa789878554973abe3c0f5a60e7b45"
          }
        },
        "a931c1e81c8349639bd08835398e115f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f564460097d44665bb83fc516c3fb8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8169dc77028945cebe5f0fccb53ec580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "42fa789878554973abe3c0f5a60e7b45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a79ba9fb6904d9fb264e155c0862aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2e2d7c12c3074eb59b662bd07286a59f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4e5a34ebdf484939a284a1e8ae2eb194",
              "IPY_MODEL_58d33de5b9cc4cc581cc20e0e4c1ee68"
            ]
          }
        },
        "2e2d7c12c3074eb59b662bd07286a59f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e5a34ebdf484939a284a1e8ae2eb194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0b71aa98bf13433cbd93cead7428c4d6",
            "_dom_classes": [],
            "description": "prediction: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 33,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 33,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34df451ca8674d78ab7baa373bde4abb"
          }
        },
        "58d33de5b9cc4cc581cc20e0e4c1ee68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_806dd934ff084da598d6e5f8e49dde36",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 33/33 [01:21&lt;00:00,  2.46s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_10037a049d024be19a827788750cd03e"
          }
        },
        "0b71aa98bf13433cbd93cead7428c4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34df451ca8674d78ab7baa373bde4abb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "806dd934ff084da598d6e5f8e49dde36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "10037a049d024be19a827788750cd03e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "98e8872874fb45ca94fd141dc92e61ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_69b7d7b9524841208903700af0d50edc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_37d5a209a5bf4128b43f74f5d95e105a",
              "IPY_MODEL_c1451229442648a7bc9a850432645a60"
            ]
          }
        },
        "69b7d7b9524841208903700af0d50edc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37d5a209a5bf4128b43f74f5d95e105a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fd8b0f32407b41cfb830c7ce2bccdf6a",
            "_dom_classes": [],
            "description": "prediction: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3532,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3532,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bce5664005554924a01397655710c449"
          }
        },
        "c1451229442648a7bc9a850432645a60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ade5d63a1ecc4b1da14263c18124cd66",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3532/3532 [5:34:38&lt;00:00,  5.68s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_075fca15dc8f456f9dc795f7b2fa5948"
          }
        },
        "fd8b0f32407b41cfb830c7ce2bccdf6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bce5664005554924a01397655710c449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ade5d63a1ecc4b1da14263c18124cd66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "075fca15dc8f456f9dc795f7b2fa5948": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POxeFOk-Rclo",
        "outputId": "1363f428-fb50-498c-dd48-ce50334deff9"
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Apr 30 12:10:45 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqKnGipFrF42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc4c280-e79a-48b4-d65f-f22e58a2b92f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kOZUSJeL-M4",
        "outputId": "36fefe43-583a-4f9e-b232-e850a94771bd"
      },
      "source": [
        "# if need bash\n",
        "!bash\n",
        "# rm -r /content/DATA\n",
        "# cp -r  /content/drive/MyDrive/ML/HW5/DATA  /content\n",
        "# cp -r  /content/DATA/DATA/rawdata/mono/mono.zh  /content/DATA/DATA/rawdata/back\n",
        "# cp -r  /content/DATA/DATA /content/drive/MyDrive/ML/DATA\n",
        "# cp -r  /content/drive/MyDrive/ML/DATA /content/DATA"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bash: cannot set terminal process group (57): Inappropriate ioctl for device\n",
            "bash: no job control in this shell\n",
            "\u001b[01;34m/content\u001b[00m# cp -r  /content/drive/MyDrive/ML/DATA /content/DATA/DATA\n",
            "cp: cannot create directory '/content/DATA/DATA': No such file or directory\n",
            "\u001b[01;34m/content\u001b[00m# cp -r  /content/drive/MyDrive/ML/DATA /content/DATA\n",
            "\u001b[01;34m/content\u001b[00m# \n",
            "\u001b[01;34m/content\u001b[00m# \n",
            "\u001b[01;34m/content\u001b[00m# \n",
            "\u001b[01;34m/content\u001b[00m# ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJLAayhkPzSe"
      },
      "source": [
        "# **Homework 5 - Sequence-to-sequence**\n",
        "\n",
        "若有任何問題，歡迎來信至助教信箱 ntu-ml-2021spring-ta@googlegroups.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmLtVdZmPzSr"
      },
      "source": [
        "### (4/14 Updates) \n",
        "1. Link to tutorial [video](https://youtu.be/htG5WpZVQPU).\n",
        "2. Now defaults to load `\"avg_last_5_checkpoint.pt\"` to generate prediction.\n",
        "3. Expected run time on Colab with Tesla T4\n",
        "\n",
        "|Baseline|Details|Total Time|\n",
        "|-|:-:|:-:|\n",
        "|Simple|2m 15s $\\times$30 epochs|1hr 8m|\n",
        "|Medium|4m $\\times$30 epochs|2hr|\n",
        "|Strong|8m $\\times$30 epochs (backward)<br>+1hr (back-translation)<br>+15m $\\times$30 epochs (forward)|12hr 30m|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsUShTcTPzSt"
      },
      "source": [
        "# Sequence-to-Sequence 介紹\n",
        "- 大多數常見的 seq2seq model 為 encoder-decoder model，主要由兩個部分組成，分別是 encoder 和 decoder，而這兩個部可以使用 recurrent neural network (RNN)或 transformer 來實作，主要是用來解決輸入和輸出的長度不一樣的情況\n",
        "- **Encoder** 是將一連串的輸入，如文字、影片、聲音訊號等，編碼為單個向量，這單個向量可以想像為是整個輸入的抽象表示，包含了整個輸入的資訊\n",
        "- **Decoder** 是將 encoder 輸出的單個向量逐步解碼，一次輸出一個結果，直到將最後目標輸出被產生出來為止，每次輸出會影響下一次的輸出，一般會在開頭加入 \"< BOS >\" 來表示開始解碼，會在結尾輸出 \"< EOS >\" 來表示輸出結束\n",
        "\n",
        "\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVOWqLKRPzSt"
      },
      "source": [
        "# 作業介紹\n",
        "- 英文翻譯中文\n",
        "  - 輸入： 一句英文 （e.g.\t\ttom is a student .） \n",
        "  - 輸出： 中文翻譯 （e.g. \t\t湯姆 是 個 學生 。）\n",
        "\n",
        "- TODO\n",
        "  - 訓練一個 RNN 模型達到 Seq2seq 翻譯\n",
        "  - 訓練一個 Transformer 大幅提升效能\n",
        "  - 實作 Back-translation 大幅提升效能"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8cWXE9qPzSu"
      },
      "source": [
        "# 下載和引入需要的函式庫"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCHeVGxmPzSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3470fb95-0070-48e4-86a3-8bbec98ee8fa"
      },
      "source": [
        "!pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "!pip install --upgrade jupyter ipywidgets"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.8MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 12.4MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/ee/d755f9e5466df64c8416a2c6a860fb3aaa43ed6ea8e8e8e81460fda5788b/wandb-0.10.28-py2.py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 53.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 57.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 15.2MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 55.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.0.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.6MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=0ee33a23fccb1c01d7c69345f9f42b1a16aa208fec4416c4b6fd856e9e12f26e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=24f07262a7fc94dbb42a2055dad6448c6c79646d8d1c834697ed83aa4667d859\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: portalocker, sacrebleu, sacremoses, sentencepiece, shortuuid, docker-pycreds, configparser, sentry-sdk, pathtools, subprocess32, smmap, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 portalocker-2.0.0 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.95 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.28\n",
            "Requirement already up-to-date: jupyter in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already up-to-date: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.6.3)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.2.0)\n",
            "Requirement already satisfied, skipping upgrade: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.6.1)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter) (4.10.1)\n",
            "Requirement already satisfied, skipping upgrade: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.0.3)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.0.5)\n",
            "Requirement already satisfied, skipping upgrade: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter) (5.3.5)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (4.7.1)\n",
            "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (0.9.4)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (22.0.3)\n",
            "Requirement already satisfied, skipping upgrade: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->jupyter-console->jupyter) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->jupyter) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exKUrNDxPzSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89c5350e-0202-428e-9a4d-9816dfec3145"
      },
      "source": [
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "!cd fairseq && git checkout 9a1c497\n",
        "!pip install --upgrade ./fairseq/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 27747, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 27747 (delta 87), reused 117 (delta 75), pack-reused 27579\u001b[K\n",
            "Receiving objects: 100% (27747/27747), 11.61 MiB | 26.48 MiB/s, done.\n",
            "Resolving deltas: 100% (20908/20908), done.\n",
            "Note: checking out '9a1c497'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 9a1c4970 Make Hydra logging work with DDP (#1568)\n",
            "Processing ./fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: numpy; python_version >= \"3.7\" in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (0.29.22)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.14.5)\n",
            "Collecting hydra-core<1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e3/fbd70dd0d3ce4d1d75c22d56c0c9f895cfa7ed6587a9ffb821d6812d6a60/hydra_core-1.0.6-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 7.9MB/s \n",
            "\u001b[?25hCollecting omegaconf<2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.5.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->fairseq==1.0.0a0+9a1c497) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+9a1c497) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: importlib-resources; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+9a1c497) (5.1.2)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 13.4MB/s \n",
            "\u001b[?25hCollecting PyYAML>=5.1.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+9a1c497) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core<1.1->fairseq==1.0.0a0+9a1c497) (3.4.1)\n",
            "Building wheels for collected packages: fairseq\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+9a1c497-cp37-cp37m-linux_x86_64.whl size=2817684 sha256=32b39c86124baec8e2b561cab2cdbc5d25c5b30792c42c0609683267cf926c61\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-b98icnwo/wheels/94/b2/67/6399f5bcb823dc3a8b1e84965aaae15af9ed863fee98a59129\n",
            "Successfully built fairseq\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp37-none-any.whl size=141231 sha256=bb520890516c3e6bb789a8b37266d19a33cb489891cfcfa7518f621ba5e17c18\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, PyYAML, omegaconf, hydra-core, fairseq\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 fairseq-1.0.0a0+9a1c497 hydra-core-1.0.6 omegaconf-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROfTbdARPzSv"
      },
      "source": [
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwUtpgyEPzSw"
      },
      "source": [
        "# 設定種子"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KJ7jBJ3PzSw"
      },
      "source": [
        "seed = 73\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)  \n",
        "np.random.seed(seed)  \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN6hXW7tPzSw"
      },
      "source": [
        "# 資料集介紹\n",
        "\n",
        "## 英轉繁雙語資料\n",
        "* [TED2020](#reimers-2020-multilingual-sentence-bert)\n",
        "    - 原始資料量: 398,066句    \n",
        "    - 處理後資料: 393,980句\n",
        "    \n",
        "\n",
        "## 測試資料\n",
        "- 資料量: 4,000句\n",
        "- **中文部分不公開，提供的檔案為假翻譯，全部都是句點。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6auPfqaPzSx"
      },
      "source": [
        "# 資料下載"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dNVMTt3PzSx"
      },
      "source": [
        "### 安裝megatools (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAd6tq46PzSx"
      },
      "source": [
        "#!apt-get install megatools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPi_tEZ-PzSy"
      },
      "source": [
        "## 下載檔案並解壓縮"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlcF6X_HPzSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd2a7d8-8512-4b30-9f1f-283b3a3e4708"
      },
      "source": [
        "data_dir = './DATA/DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "urls = (\n",
        "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214989&authkey=AGgQ-DaR8eFSl1A\"', \n",
        "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214987&authkey=AA4qP_azsicwZZM\"',\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted2020.tgz\",\n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/test.tgz\",\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\",\n",
        "#     \"https://mega.nz/#!zNcnGIoJ!oPJX9AvVVs11jc0SaK6vxP_lFUNTkEcK2WbxJpvjU5Y\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted2020.tgz', # train & dev\n",
        "    'test.tgz', # test\n",
        ")\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = prefix/f\n",
        "    if not path.exists():\n",
        "        if 'mega' in u:\n",
        "            !megadl {u} --path {path}\n",
        "        else:\n",
        "            !wget {u} -O {path}\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "!mv {prefix/'test.en'} {prefix/'test.raw.en'}\n",
        "!mv {prefix/'test.zh'} {prefix/'test.raw.zh'}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-29 15:11:54--  https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214989&authkey=AGgQ-DaR8eFSl1A\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.43.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.43.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://yva8og.dm.files.1drv.com/y4mc1EX9Upa_Or1S-Ni7E20Pbb0VdLNAa-lN3j7YtC7NJuL6274PxC3mrCxpao6-kqQ-29S9KupX3WCfZcFAPLBk8_owIJcWXu72x8keIilXcxsWbvMbqA0OiIAUYD2p-PtGt03j9BJrHqOFlwSkhKcFEDnPXvlrBMHXhlpB6IGhkmdFpA8k_DsCzBhUnRL3pdaE2ARUH87MoxAApcRq3MsTQ/ted2020.tgz?download&psid=1 [following]\n",
            "--2021-04-29 15:11:54--  https://yva8og.dm.files.1drv.com/y4mc1EX9Upa_Or1S-Ni7E20Pbb0VdLNAa-lN3j7YtC7NJuL6274PxC3mrCxpao6-kqQ-29S9KupX3WCfZcFAPLBk8_owIJcWXu72x8keIilXcxsWbvMbqA0OiIAUYD2p-PtGt03j9BJrHqOFlwSkhKcFEDnPXvlrBMHXhlpB6IGhkmdFpA8k_DsCzBhUnRL3pdaE2ARUH87MoxAApcRq3MsTQ/ted2020.tgz?download&psid=1\n",
            "Resolving yva8og.dm.files.1drv.com (yva8og.dm.files.1drv.com)... 13.107.42.12\n",
            "Connecting to yva8og.dm.files.1drv.com (yva8og.dm.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28601955 (27M) [application/x-gzip]\n",
            "Saving to: ‘/content/DATA/DATA/rawdata/ted2020/ted2020.tgz’\n",
            "\n",
            "/content/DATA/DATA/ 100%[===================>]  27.28M  13.4MB/s    in 2.0s    \n",
            "\n",
            "2021-04-29 15:11:57 (13.4 MB/s) - ‘/content/DATA/DATA/rawdata/ted2020/ted2020.tgz’ saved [28601955/28601955]\n",
            "\n",
            "raw.en\n",
            "raw.zh\n",
            "--2021-04-29 15:11:58--  https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214987&authkey=AA4qP_azsicwZZM\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.43.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.43.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://z1a8og.dm.files.1drv.com/y4mKFY5SShLPbN19eFsL69VtdabPFeKg2hWb5SRWvp-z5DE6luOaUqH1hhxWR662FzQg5JcsxXPYL_giFNp3ft4ClzRBBFEb0u4ukjeLCgqvD7xerkf0QK001LKnqXRxPiPGilVdbk1AF2Tgw5bHFvZCyMcyR-YDZdZc741KuGCCq-ijze3lhLGerYajNLbWIdaCaFpJF-jt6N7mf3E9niGcQ/test.tgz?download&psid=1 [following]\n",
            "--2021-04-29 15:11:58--  https://z1a8og.dm.files.1drv.com/y4mKFY5SShLPbN19eFsL69VtdabPFeKg2hWb5SRWvp-z5DE6luOaUqH1hhxWR662FzQg5JcsxXPYL_giFNp3ft4ClzRBBFEb0u4ukjeLCgqvD7xerkf0QK001LKnqXRxPiPGilVdbk1AF2Tgw5bHFvZCyMcyR-YDZdZc741KuGCCq-ijze3lhLGerYajNLbWIdaCaFpJF-jt6N7mf3E9niGcQ/test.tgz?download&psid=1\n",
            "Resolving z1a8og.dm.files.1drv.com (z1a8og.dm.files.1drv.com)... 13.107.42.12\n",
            "Connecting to z1a8og.dm.files.1drv.com (z1a8og.dm.files.1drv.com)|13.107.42.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 150315 (147K) [application/x-gzip]\n",
            "Saving to: ‘/content/DATA/DATA/rawdata/ted2020/test.tgz’\n",
            "\n",
            "/content/DATA/DATA/ 100%[===================>] 146.79K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2021-04-29 15:11:58 (11.0 MB/s) - ‘/content/DATA/DATA/rawdata/ted2020/test.tgz’ saved [150315/150315]\n",
            "\n",
            "test.en\n",
            "test.zh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FxD5TJdPzSz"
      },
      "source": [
        "## 設定語言"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T79gU6S5PzSz"
      },
      "source": [
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G154gPSVPzSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e24173f-4674-4ddc-ae34-947248671d5e"
      },
      "source": [
        "!head {data_prefix+'.'+src_lang} -n 5\n",
        "!head {data_prefix+'.'+tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because  I need that.\n",
            "Put yourselves in my position.\n",
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmLitTHjPzS0"
      },
      "source": [
        "## 檔案前處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCQZ690VPzS0"
      },
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "    \"\"\"把字串全形轉半形\"\"\"\n",
        "    # 參考來源:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # 全形空格直接轉換\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全形字元（除空格）根據關係轉化\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "                \n",
        "def clean_s(s, lang):\n",
        "    if lang == 'en':\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace('-', '') # remove '-'\n",
        "        s = re.sub('([.,;!?()\\\"])', r' \\1 ', s) # keep punctuation\n",
        "    elif lang == 'zh':\n",
        "        s = strQ2B(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "    return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "        print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "        return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "        with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "            with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "                with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "                    for s1 in l1_in_f:\n",
        "                        s1 = s1.strip()\n",
        "                        s2 = l2_in_f.readline().strip()\n",
        "                        s1 = clean_s(s1, l1)\n",
        "                        s2 = clean_s(s2, l2)\n",
        "                        s1_len = len_s(s1, l1)\n",
        "                        s2_len = len_s(s2, l2)\n",
        "                        if min_len > 0: # remove short sentence\n",
        "                            if s1_len < min_len or s2_len < min_len:\n",
        "                                continue\n",
        "                        if max_len > 0: # remove long sentence\n",
        "                            if s1_len > max_len or s2_len > max_len:\n",
        "                                continue\n",
        "                        if ratio > 0: # remove by ratio of length\n",
        "                            if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                                continue\n",
        "                        print(s1, file=l1_out_f)\n",
        "                        print(s2, file=l2_out_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZebF2pKbPzS3"
      },
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebw8gMrXPzS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a874fa7-3d5e-4185-84fd-770494221085"
      },
      "source": [
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n",
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc8pB-tck3bt"
      },
      "source": [
        "## save/load preprocessed data to/from gd\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnePMAr1lUTY"
      },
      "source": [
        "# save preprocessed data\n",
        "# !cp -r ./DATA /content/drive/MyDrive/ML/HW5/DATA\n",
        "#load preprocessed data\n",
        "# !cp -r /content/drive/MyDrive/ML/HW5/DATA ./DATA\n",
        "data_dir = './DATA/DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyboqOiNPzS5"
      },
      "source": [
        "## 切出 train/valid set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DL1yYpVPzS6"
      },
      "source": [
        "valid_ratio = 0.01 # 3000~4000句就夠了\n",
        "train_ratio = 1 - valid_ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KghJUznXPzS7"
      },
      "source": [
        "\n",
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "    print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "    line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "    labels = list(range(line_num))\n",
        "    random.shuffle(labels)\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "        valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "        count = 0\n",
        "        for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "            if labels[count]/line_num < train_ratio:\n",
        "                train_f.write(line)\n",
        "            else:\n",
        "                valid_f.write(line)\n",
        "            count += 1\n",
        "        train_f.close()\n",
        "        valid_f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfz1DvQnPzS7"
      },
      "source": [
        "## Subword Units \n",
        "翻譯存在的一大問題是未登錄詞(out of vocabulary)，可以使用 subword units 作為斷詞單位來解決。\n",
        "- 使用 [sentencepiece](#kudo-richardson-2018-sentencepiece) 套件\n",
        "- 用 unigram 或 byte-pair encoding (BPE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlS330taPzS8"
      },
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "    print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        input=','.join([f'{prefix}/train.clean.{src_lang}',\n",
        "                        f'{prefix}/valid.clean.{src_lang}',\n",
        "                        f'{prefix}/train.clean.{tgt_lang}',\n",
        "                        f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "        model_prefix=prefix/f'spm{vocab_size}',\n",
        "        vocab_size=vocab_size,\n",
        "        character_coverage=1,\n",
        "        model_type='unigram', # 'bpe' 也可\n",
        "        input_sentence_size=1e6,\n",
        "        shuffle_input_sentence=True,\n",
        "        normalization_rule_name='nmt_nfkc_cf',\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rO_8SJvPzS8"
      },
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'train': 'train.clean',\n",
        "    'valid': 'valid.clean',\n",
        "    'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "    for lang in [src_lang, tgt_lang]:\n",
        "        out_path = prefix/f'{split}.{lang}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "                with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = spm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8M0mAk9PzS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4685175a-c1e1-4a6c-c341-d0035a2733fe"
      },
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁ t ru ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁ ; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl own ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n",
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁ 。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁ 。 ▁我 非常 感 激 ▁ 。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對 我 之前 演講 的 好 評 ▁ 。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁ !\n",
            "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁ !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdn-lLFuPzS9"
      },
      "source": [
        "## 用 fairseq 將資料轉為 binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbpeZmgpPzS9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75d44b47-818f-4115-9621-df31c9f3e2a7"
      },
      "source": [
        "binpath = Path('./DATA/DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess \\\n",
        "        --source-lang {src_lang}\\\n",
        "        --target-lang {tgt_lang}\\\n",
        "        --trainpref {prefix/'train'}\\\n",
        "        --validpref {prefix/'valid'}\\\n",
        "        --testpref {prefix/'test'}\\\n",
        "        --destdir {binpath}\\\n",
        "        --joined-dictionary\\\n",
        "        --workers 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 15:17:15 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='DATA/DATA/data-bin/ted2020', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='zh', task='translation', tensorboard_logdir=None, testpref='/content/DATA/DATA/rawdata/ted2020/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/DATA/DATA/rawdata/ted2020/train', user_dir=None, validpref='/content/DATA/DATA/rawdata/ted2020/valid', wandb_project=None, workers=2)\n",
            "2021-04-29 15:18:25 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "2021-04-29 15:19:17 | INFO | fairseq_cli.preprocess | [en] /content/DATA/DATA/rawdata/ted2020/train.en: 390041 sents, 12426366 tokens, 0.0% replaced by <unk>\n",
            "2021-04-29 15:19:17 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "2021-04-29 15:19:17 | INFO | fairseq_cli.preprocess | [en] /content/DATA/DATA/rawdata/ted2020/valid.en: 3939 sents, 124688 tokens, 0.0% replaced by <unk>\n",
            "2021-04-29 15:19:17 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "2021-04-29 15:19:18 | INFO | fairseq_cli.preprocess | [en] /content/DATA/DATA/rawdata/ted2020/test.en: 4000 sents, 124954 tokens, 0.0% replaced by <unk>\n",
            "2021-04-29 15:19:18 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
            "2021-04-29 15:20:05 | INFO | fairseq_cli.preprocess | [zh] /content/DATA/DATA/rawdata/ted2020/train.zh: 390041 sents, 9842606 tokens, 0.0% replaced by <unk>\n",
            "2021-04-29 15:20:05 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
            "2021-04-29 15:20:05 | INFO | fairseq_cli.preprocess | [zh] /content/DATA/DATA/rawdata/ted2020/valid.zh: 3939 sents, 99156 tokens, 0.00504% replaced by <unk>\n",
            "2021-04-29 15:20:05 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
            "2021-04-29 15:20:05 | INFO | fairseq_cli.preprocess | [zh] /content/DATA/DATA/rawdata/ted2020/test.zh: 4000 sents, 12000 tokens, 0.0% replaced by <unk>\n",
            "2021-04-29 15:20:05 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to DATA/DATA/data-bin/ted2020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxLBEPFzPzS9"
      },
      "source": [
        "# 實驗的參數設定表(config)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfk7HZjbPzS9"
      },
      "source": [
        "# 1. 將實驗的參數設定表中(config)的source_lang與target_lang互相交換\n",
        "# 2. 將實驗的參數設定表中(config)的savedir更改(ex. \"./checkpoints/rnn-back\")\n",
        "# 3. 訓練一個反向模型\n",
        "\n",
        "\n",
        "# 1. 將實驗的參數設定表(config)中的datadir改為新的資料集(\"./DATA/DATA/data-bin/ted2020_with_mono\")\n",
        "# 2. 將實驗的參數設定表(config)中的source_lang與target_lang設定還原(\"en\", \"zh\")\n",
        "# 3. 將實驗的參數設定表(config)中的savedir更改(ex. \"/content/drive/MyDrive/ML/HW5/ckpt-bt/\")\n",
        "# 4. 重新訓練\n",
        "config = Namespace(\n",
        "    datadir = \"./DATA/DATA/data-bin/ted2020_with_mono\",\n",
        "    savedir = \"/content/drive/MyDrive/ML/HW5bigre/ckpt-bt/\", #backtrain modi\n",
        "    source_lang = \"en\", #backtrain modi en\n",
        "    target_lang = \"zh\", #backtrain modi zh\n",
        "    \n",
        "    # cpu threads when fetching & processing data.\n",
        "    num_workers=2,  \n",
        "    # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "    max_tokens=8192/2,\n",
        "    accum_steps=2,\n",
        "    \n",
        "    # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "    lr_factor=2.,\n",
        "    lr_warmup=4000,\n",
        "    \n",
        "    # clipping gradient norm helps alleviate gradient exploding\n",
        "    clip_norm=1.0,\n",
        "    \n",
        "    # maximum epochs for training\n",
        "    max_epoch=40,\n",
        "    start_epoch=1,\n",
        "    \n",
        "    # beam size for beam search\n",
        "    beam=5, \n",
        "    # generate sequences of maximum length ax + b, where x is the source length\n",
        "    max_len_a=1.2, \n",
        "    max_len_b=10,\n",
        "    # when decoding, post process sentence by removing sentencepiece symbols.\n",
        "    post_process = \"sentencepiece\",\n",
        "    \n",
        "    # checkpoints\n",
        "    keep_last_epochs=5,\n",
        "    resume = \"checkpoint_best.pt\",\n",
        "    # resume= 'checkpoint_last.pt', # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "    # logging\n",
        "    use_wandb=False,\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4PoynUAPzS-"
      },
      "source": [
        "# Logging\n",
        "- logging 套件紀錄一般訊息\n",
        "- wandb 紀錄續練過程 loss, bleu, model weight 等等"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EQr6FlRAPzS-"
      },
      "source": [
        "logging.basicConfig(\n",
        "    format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "    level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "    stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "    import wandb\n",
        "    wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVvQOVA2PzS-"
      },
      "source": [
        "# CUDA環境"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhhUCXVUPzS-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc17b11-b90b-46da-9d28-a7c7615e0415"
      },
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:18:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-04-30 12:18:27 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2021-04-30 12:18:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_WUk4_rPzS_"
      },
      "source": [
        "# 讀取資料集"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mCWLDf-PzS_"
      },
      "source": [
        "## 借用 fairseq 的 TranslationTask\n",
        "* 用來讀進上面 binarized 的檔案\n",
        "* 有現成的 data iterator (dataloader)\n",
        "* 字典 task.source_dictionary 和 task.target_dictionary 也很好用 \n",
        "* 有實做 beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJTK1GQqPzS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a6b488-e05c-4a91-9eb0-2ac38a5bc745"
      },
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "    data=config.datadir,\n",
        "    source_lang=config.source_lang,\n",
        "    target_lang=config.target_lang,\n",
        "    train_subset=\"train\",\n",
        "    required_seq_len_multiple=8,\n",
        "    dataset_impl=\"mmap\",\n",
        "    upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:18:29 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
            "2021-04-30 12:18:29 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiiKlvJRPzS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd420491-9891-4d51-cc43-a6849ee365d4"
      },
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:18:29 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2021-04-30 12:18:29 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/DATA/data-bin/ted2020_with_mono/train.en-zh.en\n",
            "2021-04-30 12:18:29 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/DATA/data-bin/ted2020_with_mono/train.en-zh.zh\n",
            "2021-04-30 12:18:29 | INFO | fairseq.tasks.translation | ./DATA/DATA/data-bin/ted2020_with_mono train en-zh 390041 examples\n",
            "2021-04-30 12:18:29 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/DATA/data-bin/ted2020_with_mono/train1.en-zh.en\n",
            "2021-04-30 12:18:29 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/DATA/data-bin/ted2020_with_mono/train1.en-zh.zh\n",
            "2021-04-30 12:18:29 | INFO | fairseq.tasks.translation | ./DATA/DATA/data-bin/ted2020_with_mono train1 en-zh 781713 examples\n",
            "2021-04-30 12:18:29 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/DATA/data-bin/ted2020_with_mono/valid.en-zh.en\n",
            "2021-04-30 12:18:29 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/DATA/data-bin/ted2020_with_mono/valid.en-zh.zh\n",
            "2021-04-30 12:18:29 | INFO | fairseq.tasks.translation | ./DATA/DATA/data-bin/ted2020_with_mono valid en-zh 3939 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8tbq_1XPzS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "830b98fd-67f5-47b7-bdef-ef2eb7fcb164"
      },
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "    \"Source: \" + \\\n",
        "    task.source_dictionary.string(\n",
        "        sample['source'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")\n",
        "pprint.pprint(\n",
        "    \"Target: \" + \\\n",
        "    task.target_dictionary.string(\n",
        "        sample['target'],\n",
        "        config.post_process,\n",
        "    )\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  18,   14,    6, 2218,   60,   19,   75,    4,  253,   16,  334, 1392,\n",
            "        1689,    7,    2]),\n",
            " 'target': tensor([ 145,  684,   30,  270,   40,  168, 1134,  650,  591,  367, 3117, 2417,\n",
            "        1420,  194,    2])}\n",
            "\"Source: that's exactly what i do optical mind control .\"\n",
            "'Target: 這實在就是我所做的--光學操控思想'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFIn7JiMPzS_"
      },
      "source": [
        "## Dataset Iterator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHr8gJqPPzTA"
      },
      "source": [
        "* 將每個 batch 控制在 N 個 token 讓 GPU 記憶體更有效被利用\n",
        "* 讓 training set 每個 epoch 有不同 shuffling\n",
        "* 濾掉長度太長的句子\n",
        "* 將每個 batch 內的句子 pad 成一樣長，好讓 GPU 平行運算\n",
        "* 加上 eos 並 shift 一格\n",
        "    - teacher forcing: 為了訓練模型根據prefix生成下個字，decoder的輸入會是輸出目標序列往右shift一格。\n",
        "    - 一般是會在輸入開頭加個bos token (如下圖)\n",
        "![seq2seq](https://i.imgur.com/0zeDyuI.png)\n",
        "    - fairseq 則是直接把 eos 挪到 beginning，訓練起來效果其實差不多。例如: \n",
        "    ```\n",
        "    # 輸出目標 (target) 和 Decoder輸入 (prev_output_tokens): \n",
        "                   eos = 2\n",
        "                target = 419,  711,  238,  888,  792,   60,  968,    8,    2\n",
        "    prev_output_tokens = 2,  419,  711,  238,  888,  792,   60,  968,    8\n",
        "    ```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b774N2cPzTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab659648-89a2-4e31-8e85-a1ca6aac6c93"
      },
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "    batch_iterator = task.get_batch_iterator(\n",
        "        dataset=task.dataset(split),\n",
        "        max_tokens=max_tokens,\n",
        "        max_sentences=None,\n",
        "        max_positions=utils.resolve_max_positions(\n",
        "            task.max_positions(),\n",
        "            max_tokens,\n",
        "        ),\n",
        "        ignore_invalid_inputs=True,\n",
        "        seed=seed,\n",
        "        num_workers=num_workers,\n",
        "        epoch=epoch,\n",
        "        disable_iterator_cache=not cached,\n",
        "        # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "        # first call of this method has no effect. \n",
        "    )\n",
        "    return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:18:29 | WARNING | fairseq.tasks.fairseq_task | 2,586 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[29, 2444, 135, 3058, 93, 2275, 682, 2649, 731, 1623]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': tensor([3200,  828]),\n",
              " 'net_input': {'prev_output_tokens': tensor([[   2,    4,  237, 1769,    9,  494,  491,  670,    4,   10,    1,    1,\n",
              "              1,    1,    1,    1],\n",
              "          [   2,    4,  325, 1974,  793,  294,  289,  596,    4,   10,    1,    1,\n",
              "              1,    1,    1,    1]]),\n",
              "  'src_lengths': tensor([8, 8]),\n",
              "  'src_tokens': tensor([[ 159, 2479,   17,  131,   72,  733,    7,    2],\n",
              "          [  81,    4, 1709,    4,  862,  718,    7,    2]])},\n",
              " 'nsentences': 2,\n",
              " 'ntokens': 20,\n",
              " 'target': tensor([[   4,  237, 1769,    9,  494,  491,  670,    4,   10,    2,    1,    1,\n",
              "             1,    1,    1,    1],\n",
              "         [   4,  325, 1974,  793,  294,  289,  596,    4,   10,    2,    1,    1,\n",
              "             1,    1,    1,    1]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TThDyya3PzTA"
      },
      "source": [
        "* 每個 batch 是一個字典，key 是字串，value 是 Tensor，內容說明如下\n",
        "```python\n",
        "batch = {\n",
        "    \"id\": id, # 每個 example 的 id\n",
        "    \"nsentences\": len(samples), # batch size 句子數\n",
        "    \"ntokens\": ntokens, # batch size 字數\n",
        "    \"net_input\": {\n",
        "        \"src_tokens\": src_tokens, # 來源語言的序列\n",
        "        \"src_lengths\": src_lengths, # 每句話沒有 pad 過的長度\n",
        "        \"prev_output_tokens\": prev_output_tokens, # 上面提到右 shift 一格後的目標序列\n",
        "    },\n",
        "    \"target\": target, # 目標序列\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frTt4nH8PzTA"
      },
      "source": [
        "# 定義模型架構\n",
        "* 我們一樣繼承 fairseq 的 encoder, decoder 和 model, 這樣測試階段才能直接用他寫好的 beam search 函式"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJUCzDjbPzTB"
      },
      "source": [
        "from fairseq.models import (\n",
        "    FairseqEncoder, \n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel\n",
        ")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gINK2eCMPzTB"
      },
      "source": [
        "## Encoder 編碼器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC9mi-rMPzTB"
      },
      "source": [
        "- seq2seq 模型的編碼器為 RNN 或 Transformer Encoder，以下說明以 RNN 為例，Transformer 略有不同。對於每個輸入，Encoder 會輸出一個向量和一個隱藏狀態(hidden state)，並將隱藏狀態用於下一個輸入。換句話說，Encoder 會逐步讀取輸入序列，並在每個 timestep 輸出單個向量，以及在最後 timestep 輸出最終隱藏狀態(content vector)\n",
        "- 參數:\n",
        "  - *args*\n",
        "      - encoder_embed_dim 是 embedding 的維度，主要將 one-hot vector 的單詞向量壓縮到指定的維度，主要是為了降維和濃縮資訊的功用\n",
        "      - encoder_ffn_embed_dim 是 RNN 輸出和隱藏狀態的維度(hidden dimension)\n",
        "      - encoder_layers 是 RNN 要疊多少層\n",
        "      - dropout 是決定有多少的機率會將某個節點變為 0，主要是為了防止 overfitting ，一般來說是在訓練時使用，測試時則不使用\n",
        "  - *dictionary*: fairseq 幫我們做好的 dictionary. 在此用來得到 padding index，好用來得到 encoder padding mask. \n",
        "  - *embed_tokens*: 事先做好的詞嵌入 (nn.Embedding)\n",
        "\n",
        "- 輸入: \n",
        "    - *src_tokens*: 英文的整數序列 e.g. 1, 28, 29, 205, 2 \n",
        "- 輸出: \n",
        "    - *outputs*: 最上層 RNN 每個 timestep 的輸出，後續可以用 Attention 再進行處理\n",
        "    - *final_hiddens*: 每層最終 timestep 的隱藏狀態，將傳遞到 Decoder 進行解碼\n",
        "    - *encoder_padding_mask*: 告訴我們哪些是位置的資訊不重要。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMe05SbAPzTB"
      },
      "source": [
        "class RNNEncoder(FairseqEncoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "        \n",
        "        self.embed_dim = args.encoder_embed_dim\n",
        "        self.hidden_dim = args.encoder_ffn_embed_dim\n",
        "        self.num_layers = args.encoder_layers\n",
        "        \n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim, \n",
        "            self.hidden_dim, \n",
        "            self.num_layers, \n",
        "            dropout=args.dropout, \n",
        "            batch_first=False, \n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "        self.padding_idx = dictionary.pad()\n",
        "        \n",
        "    def combine_bidir(self, outs, bsz: int):\n",
        "        out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
        "        return out.view(self.num_layers, bsz, -1)\n",
        "\n",
        "    def forward(self, src_tokens, **unused):\n",
        "        bsz, seqlen = src_tokens.size()\n",
        "        \n",
        "        # get embeddings\n",
        "        x = self.embed_tokens(src_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "        \n",
        "        # 過雙向RNN\n",
        "        h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
        "        x, final_hiddens = self.rnn(x, h0)\n",
        "        outputs = self.dropout_out_module(x)\n",
        "        # outputs = [sequence len, batch size, hid dim * directions] 是最上層RNN的輸出\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        \n",
        "        # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\n",
        "        final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
        "        # hidden =  [num_layers x batch x num_directions*hidden]\n",
        "        \n",
        "        encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
        "        return tuple(\n",
        "            (\n",
        "                outputs,  # seq_len x batch x hidden\n",
        "                final_hiddens,  # num_layers x batch x num_directions*hidden\n",
        "                encoder_padding_mask,  # seq_len x batch\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    def reorder_encoder_out(self, encoder_out, new_order):\n",
        "        # 這個beam search時會用到，意義並不是很重要\n",
        "        return tuple(\n",
        "            (\n",
        "                encoder_out[0].index_select(1, new_order),\n",
        "                encoder_out[1].index_select(1, new_order),\n",
        "                encoder_out[2].index_select(1, new_order),\n",
        "            )\n",
        "        )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3hC9aODPzTB"
      },
      "source": [
        "## Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQqi69c6PzTC"
      },
      "source": [
        "- 當輸入過長，或是單獨靠 “content vector” 無法取得整個輸入的意思時，用 Attention Mechanism 來提供 Decoder 更多的資訊\n",
        "- 根據現在 **Decoder embeddings** ，去計算在 **Encoder outputs** 中，那些與其有較高的關係，根據關係的數值來把 Encoder outputs 平均起來作為 **Decoder** RNN 的輸入 \n",
        "- 常見 Attention 的實作是用 Neural Network / Dot Product 來算 **query** (decoder embeddings) 和 **key** (Encoder outputs) 之間的關係，再對所有算出來的數值做 **softmax** 得到分佈，最後根據這個分佈對 **values** (Encoder outputs) 做 **weight sum**\n",
        "\n",
        "- 參數:\n",
        "  - *input_embed_dim*: key 的維度，應是 decoder 要做 attend 時的向量的維度\n",
        "  - *source_embed_dim*: query 的維度，應是要被 attend 的向量(encoder outputs)的維度\n",
        "  - *output_embed_dim*: value 的維度，應是做完 attention 後，下一層預期的向量維度\n",
        "\n",
        "- 輸入: \n",
        "    - *inputs*: 就是 key，要 attend 別人的向量\n",
        "    - *encoder_outputs*: 是 query/value，被 attend 的向量\n",
        "    - *encoder_padding_mask*: 告訴我們哪些是位置的資訊不重要。\n",
        "- 輸出: \n",
        "    - *output*: 做完 attention 後的 context vector\n",
        "    - *attention score*: attention 的分布\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYLDfbkMPzTC"
      },
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
        "        self.output_proj = nn.Linear(\n",
        "            input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
        "        # inputs: T, B, dim\n",
        "        # encoder_outputs: S x B x dim\n",
        "        # padding mask:  S x B\n",
        "        \n",
        "        # convert all to batch first\n",
        "        inputs = inputs.transpose(1,0) # B, T, dim\n",
        "        encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
        "        encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
        "        \n",
        "        # 投影到encoder_outputs的維度\n",
        "        x = self.input_proj(inputs)\n",
        "\n",
        "        # 計算attention\n",
        "        # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
        "        attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
        "\n",
        "        # 擋住padding位置的attention\n",
        "        if encoder_padding_mask is not None:\n",
        "            # 利用broadcast  B, S -> (B, 1, S)\n",
        "            encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
        "            attn_scores = (\n",
        "                attn_scores.float()\n",
        "                .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
        "                .type_as(attn_scores)\n",
        "            )  # FP16 support: cast to float and back\n",
        "\n",
        "        # 在source對應維度softmax\n",
        "        attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # 形狀 (B, T, S) x (B, S, dim) = (B, T, dim) 加權平均\n",
        "        x = torch.bmm(attn_scores, encoder_outputs)\n",
        "\n",
        "        # (B, T, dim)\n",
        "        x = torch.cat((x, inputs), dim=-1)\n",
        "        x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
        "        \n",
        "        # 回復形狀 (B, T, dim) -> (T, B, dim)\n",
        "        return x.transpose(1,0), attn_scores"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eCELGqKPzTC"
      },
      "source": [
        "## Decoder 解碼器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES-8rtnePzTC"
      },
      "source": [
        "* 解碼器的 hidden states 會用編碼器最終隱藏狀態來初始化(content vector)\n",
        "* 解碼器同時也根據目前 timestep 的輸入(也就是前幾個 timestep 的 output)，改變 hidden states，並輸出結果 \n",
        "* 如果加入 attention 可以使表現更好\n",
        "* 我們把 seq2seq 步驟寫在解碼器裡，好讓等等 Seq2Seq 這個型別可以通用 RNN 和 Transformer，而不用再改寫\n",
        "- 參數:\n",
        "  - *args*\n",
        "      - decoder_embed_dim 是解碼器 embedding 的維度，類同 encoder_embed_dim，\n",
        "      - decoder_ffn_embed_dim 是解碼器 RNN 的隱藏維度，類同 encoder_ffn_embed_dim\n",
        "      - decoder_layers 解碼器 RNN 的層數\n",
        "      - share_decoder_input_output_embed 通常 decoder 最後輸出的投影矩陣會和輸入 embedding 共用參數\n",
        "  - *dictionary*: fairseq 幫我們做好的 dictionary.\n",
        "  - *embed_tokens*: 事先做好的詞嵌入(nn.Embedding)\n",
        "- 輸入: \n",
        "    - *prev_output_tokens*: 英文的整數序列 e.g. 1, 28, 29, 205, 2 已經 shift 一格的 target\n",
        "    - *encoder_out*: 編碼器的輸出\n",
        "    - *incremental_state*: 這是測試階段為了加速，所以會記錄每個 timestep 的 hidden state 詳見 forward\n",
        "- 輸出: \n",
        "    - *outputs*: decoder 每個 timestep 的 logits，還沒經過 softmax 的分布\n",
        "    - *extra*: 沒用到"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eob9S51mPzTC"
      },
      "source": [
        "class RNNDecoder(FairseqIncrementalDecoder):\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.embed_tokens = embed_tokens\n",
        "        \n",
        "        assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
        "        and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
        "        assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
        "        that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
        "        \n",
        "        self.embed_dim = args.decoder_embed_dim\n",
        "        self.hidden_dim = args.decoder_ffn_embed_dim\n",
        "        self.num_layers = args.decoder_layers\n",
        "        \n",
        "        \n",
        "        self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "        self.rnn = nn.GRU(\n",
        "            self.embed_dim, \n",
        "            self.hidden_dim, \n",
        "            self.num_layers, \n",
        "            dropout=args.dropout, \n",
        "            batch_first=False, \n",
        "            bidirectional=False\n",
        "        )\n",
        "        self.attention = AttentionLayer(\n",
        "            self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
        "        ) \n",
        "        # self.attention = None\n",
        "        self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "        if self.hidden_dim != self.embed_dim:\n",
        "            self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
        "        else:\n",
        "            self.project_out_dim = None\n",
        "        \n",
        "        if args.share_decoder_input_output_embed:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.embed_tokens.weight.shape[1],\n",
        "                self.embed_tokens.weight.shape[0],\n",
        "                bias=False,\n",
        "            )\n",
        "            self.output_projection.weight = self.embed_tokens.weight\n",
        "        else:\n",
        "            self.output_projection = nn.Linear(\n",
        "                self.output_embed_dim, len(dictionary), bias=False\n",
        "            )\n",
        "            nn.init.normal_(\n",
        "                self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
        "            )\n",
        "        \n",
        "    def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
        "        # 取出encoder的輸出\n",
        "        encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
        "        # outputs:          seq_len x batch x num_directions*hidden\n",
        "        # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
        "        # padding_mask:     seq_len x batch\n",
        "        \n",
        "        if incremental_state is not None and len(incremental_state) > 0:\n",
        "            # 有上個timestep留下的資訊，讀進來就可以繼續decode，不用從bos重來\n",
        "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
        "            cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "            prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        else:\n",
        "            # 沒有incremental state代表這是training或者是test time時的第一步\n",
        "            # 準備seq2seq: 把encoder_hiddens pass進去decoder的hidden states\n",
        "            prev_hiddens = encoder_hiddens\n",
        "        \n",
        "        bsz, seqlen = prev_output_tokens.size()\n",
        "        \n",
        "        # embed tokens\n",
        "        x = self.embed_tokens(prev_output_tokens)\n",
        "        x = self.dropout_in_module(x)\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "                \n",
        "        # 做decoder-to-encoder attention\n",
        "        if self.attention is not None:\n",
        "            x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
        "                        \n",
        "        # 過單向RNN\n",
        "        x, final_hiddens = self.rnn(x, prev_hiddens)\n",
        "        # outputs = [sequence len, batch size, hid dim]\n",
        "        # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        x = self.dropout_out_module(x)\n",
        "                \n",
        "        # 投影到embedding size (如果hidden 和embed size不一樣，然後share_embedding又設成True,需要額外project一次)\n",
        "        if self.project_out_dim != None:\n",
        "            x = self.project_out_dim(x)\n",
        "        \n",
        "        # 投影到vocab size 的分佈\n",
        "        x = self.output_projection(x)\n",
        "        \n",
        "        # T x B x C -> B x T x C\n",
        "        x = x.transpose(1, 0)\n",
        "        \n",
        "        # 如果是Incremental, 記錄這個timestep的hidden states, 下個timestep讀回來\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": final_hiddens,\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        \n",
        "        return x, None\n",
        "    \n",
        "    def reorder_incremental_state(\n",
        "        self,\n",
        "        incremental_state,\n",
        "        new_order,\n",
        "    ):\n",
        "        # 這個beam search時會用到，意義並不是很重要\n",
        "        cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "        prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "        prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
        "        cache_state = {\n",
        "            \"prev_hiddens\": torch.stack(prev_hiddens),\n",
        "        }\n",
        "        self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        return"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTerxx_2PzTD"
      },
      "source": [
        "## Seq2Seq\n",
        "- 由 **Encoder** 和 **Decoder** 組成\n",
        "- 接收輸入並傳給 **Encoder** \n",
        "- 將 **Encoder** 的輸出傳給 **Decoder**\n",
        "- **Decoder** 根據前幾個 timestep 的輸出和 **Encoder** 輸出進行解碼  \n",
        "- 當解碼完成後，將 **Decoder** 的輸出傳回 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqZRnIuxPzTD"
      },
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "    def __init__(self, args, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "        self.args = args\n",
        "    \n",
        "    def forward(\n",
        "        self,\n",
        "        src_tokens,\n",
        "        src_lengths,\n",
        "        prev_output_tokens,\n",
        "        return_all_hiddens: bool = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(\n",
        "            src_tokens, src_lengths=src_lengths, return_all_hiddens=return_all_hiddens\n",
        "        )\n",
        "        logits, extra = self.decoder(\n",
        "            prev_output_tokens,\n",
        "            encoder_out=encoder_out,\n",
        "            src_lengths=src_lengths,\n",
        "            return_all_hiddens=return_all_hiddens,\n",
        "        )\n",
        "        return logits, extra"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-9XkN3bPzTD"
      },
      "source": [
        "# 模型初始化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYpOstBzPzTD"
      },
      "source": [
        "# # HINT: transformer 架構\n",
        "from fairseq.models.transformer import (\n",
        "    TransformerEncoder, \n",
        "    TransformerDecoder,\n",
        ")\n",
        "\n",
        "def build_model(args, task):\n",
        "    \"\"\" 按照參數設定建置模型 \"\"\"\n",
        "    src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "    # 詞嵌入\n",
        "    encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "    decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "    \n",
        "    # 編碼器與解碼器\n",
        "    # TODO: 替換成 TransformerEncoder 和 TransformerDecoder\n",
        "    encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "    decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "    \n",
        "    # 序列到序列模型\n",
        "    model = Seq2Seq(args, encoder, decoder)\n",
        "    \n",
        "    # 序列到序列模型的初始化很重要 需要特別處理\n",
        "    def init_params(module):\n",
        "        from fairseq.modules import MultiheadAttention\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        if isinstance(module, MultiheadAttention):\n",
        "            module.q_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.k_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            module.v_proj.weight.data.normal_(mean=0.0, std=0.02)\n",
        "        if isinstance(module, nn.RNNBase):\n",
        "            for name, param in module.named_parameters():\n",
        "                if \"weight\" in name or \"bias\" in name:\n",
        "                    param.data.uniform_(-0.1, 0.1)\n",
        "            \n",
        "    # 初始化模型\n",
        "    model.apply(init_params)\n",
        "    return model"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJH_vv4WPzTE"
      },
      "source": [
        "## 設定模型相關參數\n",
        "參考參數\n",
        "\n",
        "|model|embedding dim|encoder ffn|encoder layers|decoder ffn|decoder layers|\n",
        "|-|-|-|-|-|-|\n",
        "|RNN|256|512|1|1024|1|\n",
        "|Transformer|256|1024|4|1024|4|\n",
        "\n",
        "Strong baseline 用的參數可以參考 [Attention is all you need](#vaswani2017) 的 Table 3 的 transformer-base\n",
        "\n",
        "\n",
        "![gg.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TvwP57IPzTE"
      },
      "source": [
        "arch_args = Namespace(\n",
        "    encoder_embed_dim=1024, #256\n",
        "    encoder_ffn_embed_dim=4096, #1024\n",
        "    encoder_layers=6, #4\n",
        "    decoder_embed_dim=1024, #256\n",
        "    decoder_ffn_embed_dim=4096, #1024\n",
        "    decoder_layers=6, #4\n",
        "    share_decoder_input_output_embed=True,\n",
        "    dropout=0.3, #0.3\n",
        ")\n",
        "\n",
        "    # HINT: 補上Transformer用的參數\n",
        "def add_transformer_args(args):\n",
        "    args.encoder_attention_heads=8 #4\n",
        "    args.encoder_normalize_before=True\n",
        "    \n",
        "    args.decoder_attention_heads=8 #4\n",
        "    args.decoder_normalize_before=True\n",
        "    \n",
        "    args.activation_fn=\"relu\"\n",
        "    args.max_source_positions=1024\n",
        "    args.max_target_positions=1024\n",
        "    \n",
        "    # 補上我們沒有設定的Transformer預設參數\n",
        "    from fairseq.models.transformer import base_architecture \n",
        "    base_architecture(arch_args)\n",
        "\n",
        "add_transformer_args(arch_args)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v7WjSdePzTE"
      },
      "source": [
        "if config.use_wandb:\n",
        "    wandb.config.update(vars(arch_args))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJsziqWVPzTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09ef525d-9f36-4b68-d00a-63bcb0d7f100"
      },
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:18:36 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=1024, out_features=8000, bias=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_O9kcP_DPzTF"
      },
      "source": [
        "# Optimization 最佳化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfV5PgCTPzTG"
      },
      "source": [
        "## Loss: Label Smoothing Regularization\n",
        "* 讓模型學習輸出較不集中的分佈，防止模型過度自信\n",
        "* 有時候Ground Truth並非唯一答案，所以在算loss時，我們會保留一部份機率給正確答案以外的label\n",
        "* 可以有效防止過度擬合\n",
        "\n",
        "code [source](https://fairseq.readthedocs.io/en/latest/_modules/fairseq/criterions/label_smoothed_cross_entropy.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHnZnhx6PzTG"
      },
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "    def __init__(self, smoothing, ignore_index=None, reduce=True):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reduce = reduce\n",
        "    \n",
        "    def forward(self, lprobs, target):\n",
        "        if target.dim() == lprobs.dim() - 1:\n",
        "            target = target.unsqueeze(-1)\n",
        "        # nll: Negative log likelihood，當目標是one-hot時的cross-entropy loss. 以下同 F.nll_loss\n",
        "        nll_loss = -lprobs.gather(dim=-1, index=target)\n",
        "        # 將一部分正確答案的機率分配給其他label 所以當計算cross-entropy時等於把所有label的log prob加起來\n",
        "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "        if self.ignore_index is not None:\n",
        "            pad_mask = target.eq(self.ignore_index)\n",
        "            nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "            smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "        else:\n",
        "            nll_loss = nll_loss.squeeze(-1)\n",
        "            smooth_loss = smooth_loss.squeeze(-1)\n",
        "        if self.reduce:\n",
        "            nll_loss = nll_loss.sum()\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "        # 計算cross-entropy時 加入分配給其他label的loss\n",
        "        eps_i = self.smoothing / lprobs.size(-1)\n",
        "        loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "        return loss\n",
        "\n",
        "# 一般都用0.1效果就很好了\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(\n",
        "    smoothing=0.1,\n",
        "    ignore_index=task.target_dictionary.pad(),\n",
        ")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If54jC_CPzTG"
      },
      "source": [
        "## Optimizer: Adam + lr scheduling\n",
        "Inverse square root 排程對於訓練 Transformer 時的穩定性很重要，後來也用在 RNN 上。\n",
        "根據底下公式來更新 learning rate，前期線性增長，後期根據更新步數方根的倒數來遞減。\n",
        "$$lrate = d_{\\text{model}}^{-0.5}\\cdot\\min({step\\_num}^{-0.5},{step\\_num}\\cdot{warmup\\_steps}^{-1.5})$$\n",
        "code [source](https://nlp.seas.harvard.edu/2018/04/03/attention.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdA1T9glPzTG"
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    @property\n",
        "    def param_groups(self):\n",
        "        return self.optimizer.param_groups\n",
        "        \n",
        "    def multiply_grads(self, c):\n",
        "        \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is not None:\n",
        "                    p.grad.data.mul_(c)\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return 0 if not step else self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUrTWHfDPzTG"
      },
      "source": [
        "## 排程視覺化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcyBr5wbPzTH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3eb5b15a-b10b-468f-f9f1-0e86800da332"
      },
      "source": [
        "optimizer = NoamOpt(\n",
        "    model_size=arch_args.encoder_embed_dim, \n",
        "    factor=config.lr_factor, \n",
        "    warmup=config.lr_warmup, \n",
        "    optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPzaBnD0PzTH"
      },
      "source": [
        "# 訓練步驟"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGQV2TOtPzTH"
      },
      "source": [
        "## Training 訓練"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khRJP5LcPzTH"
      },
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps=1):\n",
        "    itr = epoch_itr.next_epoch_itr(shuffle=True)\n",
        "    itr = iterators.GroupedIterator(itr, accum_steps) # 梯度累積: 每 accum_steps 個 sample 更新一次\n",
        "    \n",
        "    stats = {\"loss\": []}\n",
        "    scaler = GradScaler() # 混和精度訓練 automatic mixed precision (amp) \n",
        "    \n",
        "    model.train()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"train epoch {epoch_itr.epoch}\", leave=False)\n",
        "    for samples in progress:\n",
        "        model.zero_grad()\n",
        "        accum_loss = 0\n",
        "        sample_size = 0\n",
        "        # 梯度累積: 每 accum_steps 個 sample 更新一次\n",
        "        for i, sample in enumerate(samples):\n",
        "            if i == 1:\n",
        "                # emptying the CUDA cache after the first step can reduce the chance of OOM\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size_i = sample[\"ntokens\"]\n",
        "            sample_size += sample_size_i\n",
        "            \n",
        "            # 混和精度訓練 \n",
        "            with autocast():\n",
        "                net_output = model.forward(**sample[\"net_input\"])\n",
        "                lprobs = F.log_softmax(net_output[0], -1)            \n",
        "                loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "                \n",
        "                # logging\n",
        "                accum_loss += loss.item()\n",
        "                # back-prop\n",
        "                scaler.scale(loss).backward()                \n",
        "        \n",
        "        scaler.unscale_(optimizer)\n",
        "        optimizer.multiply_grads(1 / (sample_size or 1.0)) # (sample_size or 1.0) handles the case of a zero gradient\n",
        "        gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) # 梯度裁剪 防止梯度爆炸\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "        # logging\n",
        "        loss_print = accum_loss/sample_size\n",
        "        stats[\"loss\"].append(loss_print)\n",
        "        progress.set_postfix(loss=loss_print)\n",
        "        if config.use_wandb:\n",
        "            wandb.log({\n",
        "                \"train/loss\": loss_print,\n",
        "                \"train/grad_norm\": gnorm.item(),\n",
        "                \"train/lr\": optimizer.rate(),\n",
        "                \"train/sample_size\": sample_size,\n",
        "            })\n",
        "        \n",
        "    loss_print = np.mean(stats[\"loss\"])\n",
        "    logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "    return stats"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edM6C5JtPzTI"
      },
      "source": [
        "## Validation & Inference 檢驗和推論\n",
        "為防止訓練發生過度擬合，每過一段時間要做一次檢測，計算模型在未看過的資料上的表現。\n",
        "- 過程基本上和training一樣，另外加上 inference\n",
        "- 檢驗完畢可順便儲存模型參數\n",
        "\n",
        "單看 validation loss，我們很難知道模型真實的效能\n",
        "- 直接用當前模型去生成翻譯結果 (hypothesis)，再和正確答案 (reference) 計算 BLEU score\n",
        "- 也可用肉眼看翻譯結果的好壞\n",
        "- 我們用 fairseq 寫好的 sequence generator 來進行 beam search 生成翻譯結果"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdgPuDyWPzTI"
      },
      "source": [
        "# fairseq 的 beam search generator\n",
        "# 給定模型和輸入序列，用 beam search 生成翻譯結果\n",
        "sequence_generator = task.build_generator([model], config)\n",
        "\n",
        "def decode(toks, dictionary):\n",
        "    # 從 Tensor 轉成人看得懂的句子\n",
        "    s = dictionary.string(\n",
        "        toks.int().cpu(),\n",
        "        config.post_process,\n",
        "    )\n",
        "    return s if s else \"<unk>\"\n",
        "\n",
        "def inference_step(sample, model):\n",
        "    gen_out = sequence_generator.generate([model], sample)\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    for i in range(len(gen_out)):\n",
        "        # 對於每個 sample, 收集輸入，輸出和參考答案，稍後計算 BLEU\n",
        "        srcs.append(decode(\n",
        "            utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()), \n",
        "            task.source_dictionary,\n",
        "        ))\n",
        "        hyps.append(decode(\n",
        "            gen_out[i][0][\"tokens\"], # 0 代表取出 beam 內分數第一的輸出結果\n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "        refs.append(decode(\n",
        "            utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()), \n",
        "            task.target_dictionary,\n",
        "        ))\n",
        "    return srcs, hyps, refs"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euR1MFMnPzTJ"
      },
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb=True):\n",
        "    logger.info('begin validation')\n",
        "    itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    stats = {\"loss\":[], \"bleu\": 0, \"srcs\":[], \"hyps\":[], \"refs\":[]}\n",
        "    srcs = []\n",
        "    hyps = []\n",
        "    refs = []\n",
        "    \n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"validation\", leave=False)\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "            net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "            lprobs = F.log_softmax(net_output[0], -1)\n",
        "            target = sample[\"target\"]\n",
        "            sample_size = sample[\"ntokens\"]\n",
        "            loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "            progress.set_postfix(valid_loss=loss.item())\n",
        "            stats[\"loss\"].append(loss)\n",
        "            \n",
        "            # 進行推論\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            srcs.extend(s)\n",
        "            hyps.extend(h)\n",
        "            refs.extend(r)\n",
        "            \n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacrebleu.corpus_bleu(hyps, [refs], tokenize=tok) # 計算BLEU score\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "    \n",
        "    if config.use_wandb and log_to_wandb:\n",
        "        wandb.log({\n",
        "            \"valid/loss\": stats[\"loss\"],\n",
        "            \"valid/bleu\": stats[\"bleu\"].score,\n",
        "        }, commit=False)\n",
        "    \n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source: \" + srcs[showid])\n",
        "    logger.info(\"example hypothesis: \" + hyps[showid])\n",
        "    logger.info(\"example reference: \" + refs[showid])\n",
        "    \n",
        "    # show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOI2K6_pPzTJ"
      },
      "source": [
        "# 儲存及載入模型參數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxqaS3ZaPzTM"
      },
      "source": [
        "def validate_and_save(model, task, criterion, optimizer, epoch, save=True):   \n",
        "    stats = validate(model, task, criterion)\n",
        "    bleu = stats['bleu']\n",
        "    loss = stats['loss']\n",
        "    if save:\n",
        "        # save epoch checkpoints\n",
        "        savedir = Path(config.savedir).absolute()\n",
        "        savedir.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        check = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "            \"optim\": {\"step\": optimizer._step}\n",
        "        }\n",
        "        torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "        shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "        logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "    \n",
        "        # save epoch samples\n",
        "        with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "            for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "                f.write(f\"{s}\\t{h}\\n\")\n",
        "\n",
        "        # get best valid bleu    \n",
        "        if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "            validate_and_save.best_bleu = bleu.score\n",
        "            torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "            \n",
        "        del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "        if del_file.exists():\n",
        "            del_file.unlink()\n",
        "    \n",
        "    return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "    name = name if name else \"checkpoint_last.pt\"\n",
        "    checkpath = Path(config.savedir)/name\n",
        "    if checkpath.exists():\n",
        "        check = torch.load(checkpath)\n",
        "        model.load_state_dict(check[\"model\"])\n",
        "        stats = check[\"stats\"]\n",
        "        step = \"unknown\"\n",
        "        if optimizer != None:\n",
        "            optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "        logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "    else:\n",
        "        logger.info(f\"no checkpoints found at {checkpath}!\")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXogchICPzTN"
      },
      "source": [
        "# 主程式\n",
        "## 訓練迴圈"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mD1jutNTPzTO"
      },
      "source": [
        "\n",
        "model = model.to(device=device)\n",
        "criterion = criterion.to(device=device)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFt0vexLPzTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a446125d-1ff2-4a65-b5a1-c4fd7eb97621"
      },
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "    \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "        sum(p.numel() for p in model.parameters()),\n",
        "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:18:48 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2021-04-30 12:18:48 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2021-04-30 12:18:48 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2021-04-30 12:18:48 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2021-04-30 12:18:48 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2021-04-30 12:18:48 | INFO | hw5.seq2seq | num. model params: 192,745,472 (num. trained: 192,745,472)\n",
            "2021-04-30 12:18:48 | INFO | hw5.seq2seq | max tokens per batch = 4096.0, accumulate steps = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gO0jw8f1PzTO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453,
          "referenced_widgets": [
            "2367de31cd134581b1d69be9ff9c5777",
            "1e9a9edd72a041758d24656fb3eed7cf",
            "494b6e25805a43938dd5885b9ccd3f04",
            "d54a4fb73d984c6f80565dbc2052c11e",
            "970ec52792784da9b97d54b5fef1ec04",
            "fe44711eae42413dabf6aae3550d399a",
            "5266710090474663bc876d53b07da34b",
            "0deda2826060469ba6e073b5d62e041a"
          ]
        },
        "outputId": "8e2677b9-38b7-4a54-9c4c-484135a4ccd6"
      },
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "    # train for one epoch\n",
        "    for param_group in optimizer.param_groups:\n",
        "      print(param_group['lr'])\n",
        "    train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "    stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "    logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "    epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:18:48 | WARNING | fairseq.tasks.fairseq_task | 1 samples have invalid sizes and will be skipped, max_positions=(1024, 1024), first few sample ids=[326674]\n",
            "2021-04-30 12:19:03 | INFO | hw5.seq2seq | loaded checkpoint /content/drive/MyDrive/ML/HW5bigre/ckpt-bt/checkpoint_best.pt: step=98235 loss=2.804790496826172 bleu=31.613941906359628\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2367de31cd134581b1d69be9ff9c5777",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='train epoch 1', max=3790.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-4bae98aae3eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_itr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"end of epoch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-e3d66598e8c4>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_itr, model, task, criterion, optimizer, accum_steps)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;31m# logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0maccum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0;31m# back-prop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9Y6vjm6PzTO"
      },
      "source": [
        "# Submission 繳交檔案"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiW_NjXJPzTP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ebc369-a9e3-4a56-9fbd-035747fa57e5"
      },
      "source": [
        "# 把幾個 checkpoint 平均起來可以達到 ensemble 的效果\n",
        "checkdir=config.savedir\n",
        "!python ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(checkpoint_upper_bound=None, inputs=['/content/drive/MyDrive/ML/HW5bigre/ckpt-bt/'], num_epoch_checkpoints=5, num_update_checkpoints=None, output='/content/drive/MyDrive/ML/HW5bigre/ckpt-bt//avg_last_5_checkpoint.pt')\n",
            "averaging checkpoints:  ['/content/drive/MyDrive/ML/HW5bigre/ckpt-bt/checkpoint27.pt', '/content/drive/MyDrive/ML/HW5bigre/ckpt-bt/checkpoint26.pt', '/content/drive/MyDrive/ML/HW5bigre/ckpt-bt/checkpoint25.pt', '/content/drive/MyDrive/ML/HW5bigre/ckpt-bt/checkpoint24.pt', '/content/drive/MyDrive/ML/HW5bigre/ckpt-bt/checkpoint23.pt']\n",
            "Finished writing averaged checkpoint to /content/drive/MyDrive/ML/HW5bigre/ckpt-bt//avg_last_5_checkpoint.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFddYalrPzTP"
      },
      "source": [
        "## 確認生成繳交檔案的模型參數"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLn_cvbZPzTP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "e32030000c8c499e8708ea71b217ae73",
            "86708699449b492ba06ca666c1321659",
            "4d4bf0bbf5ec498cafa273dcd9c7dc8c",
            "b7e2216fa89c4f57939b4659343881d5",
            "a931c1e81c8349639bd08835398e115f",
            "f564460097d44665bb83fc516c3fb8bf",
            "8169dc77028945cebe5f0fccb53ec580",
            "42fa789878554973abe3c0f5a60e7b45"
          ]
        },
        "outputId": "cf527fa8-2e6d-456e-961a-2062b73d82f5"
      },
      "source": [
        "# checkpoint_last.pt : 最後一次檢驗的檔案\n",
        "# checkpoint_best.pt : 檢驗 BLEU 最高的檔案\n",
        "# avg_last_3_checkpoint.pt:　最5後個檔案平均\n",
        "try_load_checkpoint(model, name=\"avg_last_5_checkpoint.pt\")\n",
        "validate(model, task, criterion, log_to_wandb=False)\n",
        "None"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:29:34 | INFO | hw5.seq2seq | loaded checkpoint /content/drive/MyDrive/ML/HW5bigre/ckpt-bt/avg_last_5_checkpoint.pt: step=unknown loss=2.80202317237854 bleu=31.41628441124407\n",
            "2021-04-30 12:29:34 | INFO | hw5.seq2seq | begin validation\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e32030000c8c499e8708ea71b217ae73",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='validation', max=41.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:31:18 | INFO | hw5.seq2seq | example source: thank you . thank you .\n",
            "2021-04-30 12:31:18 | INFO | hw5.seq2seq | example hypothesis: 謝謝 。 謝謝 。\n",
            "2021-04-30 12:31:18 | INFO | hw5.seq2seq | example reference: 謝謝 。 謝謝 。\n",
            "2021-04-30 12:31:18 | INFO | hw5.seq2seq | validation loss:\t2.7796\n",
            "2021-04-30 12:31:18 | INFO | hw5.seq2seq | BLEU = 31.87 61.5/38.2/25.2/17.6 (BP = 0.997 ratio = 0.997 hyp_len = 111488 ref_len = 111811)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNfFpKoFPzTP"
      },
      "source": [
        "## 進行預測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeBxh_PMPzTP"
      },
      "source": [
        "def generate_prediction(model, task, split=\"test\", outfile=\"./prediction.txt\",outfilegd =\"/content/drive/MyDrive/ML/HW5/pred/prediction.txt\"):    \n",
        "    task.load_dataset(split=split, epoch=1)\n",
        "    itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "    idxs = []\n",
        "    hyps = []\n",
        "\n",
        "    model.eval()\n",
        "    progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "    with torch.no_grad():\n",
        "        for i, sample in enumerate(progress):\n",
        "            # validation loss\n",
        "            sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "            # 進行推論\n",
        "            s, h, r = inference_step(sample, model)\n",
        "            \n",
        "            hyps.extend(h)\n",
        "            idxs.extend(list(sample['id']))\n",
        "            \n",
        "    # 根據 preprocess 時的順序排列\n",
        "    hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    \n",
        "    with open(outfile, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")\n",
        "    with open(outfilegd, \"w\") as f:\n",
        "        for h in hyps:\n",
        "            f.write(h+\"\\n\")"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBQXiTuiPzTP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137,
          "referenced_widgets": [
            "2a79ba9fb6904d9fb264e155c0862aa1",
            "2e2d7c12c3074eb59b662bd07286a59f",
            "4e5a34ebdf484939a284a1e8ae2eb194",
            "58d33de5b9cc4cc581cc20e0e4c1ee68",
            "0b71aa98bf13433cbd93cead7428c4d6",
            "34df451ca8674d78ab7baa373bde4abb",
            "806dd934ff084da598d6e5f8e49dde36",
            "10037a049d024be19a827788750cd03e"
          ]
        },
        "outputId": "55a76726-6ea4-4730-8a5d-76c8c74879d0"
      },
      "source": [
        "generate_prediction(model, task)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-30 12:31:40 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/DATA/data-bin/ted2020_with_mono/test.en-zh.en\n",
            "2021-04-30 12:31:40 | INFO | fairseq.data.data_utils | loaded 4,000 examples from: ./DATA/DATA/data-bin/ted2020_with_mono/test.en-zh.zh\n",
            "2021-04-30 12:31:40 | INFO | fairseq.tasks.translation | ./DATA/DATA/data-bin/ted2020_with_mono test en-zh 4000 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a79ba9fb6904d9fb264e155c0862aa1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='prediction', max=33.0, style=ProgressStyle(description_wi…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqu3l35sPzTQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "25552ba2-1486-4d61-ffc6-b02e2bc30389"
      },
      "source": [
        "raise"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-9c9a2cba73bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7933Zu9ayHIY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gciTr_qUPzTQ"
      },
      "source": [
        "# Back-translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipi0gIwsPzTQ"
      },
      "source": [
        "## 訓練一個反向的翻譯模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQg8Fae8PzTQ"
      },
      "source": [
        "1. 將實驗的參數設定表中(config)的source_lang與target_lang互相交換\n",
        "2. 將實驗的參數設定表中(config)的savedir更改(ex. \"./checkpoints/rnn-back\")\n",
        "3. 訓練一個反向模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73QNgfxEPzTQ"
      },
      "source": [
        "## 利用反向模型生成額外資料"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLTcbPWOPzTR"
      },
      "source": [
        "### 下載 monolingual data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVKD1ZsrPzTR"
      },
      "source": [
        "mono_dataset_name = 'mono'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcxnwxvwPzTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd44b50-bc6d-4c45-e349-3589d7d6432e"
      },
      "source": [
        "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "print(mono_prefix)\n",
        "mono_prefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "urls = (\n",
        "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214986&authkey=AANUKbGfZx0kM80\"',\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted_zh_corpus.deduped.gz\",\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://mega.nz/#!vMNnDShR!4eHDxzlpzIpdpeQTD-htatU_C7QwcBTwGDaSeBqH534\",\n",
        ")\n",
        "file_names = (\n",
        "    'ted_zh_corpus.deduped.gz',\n",
        ")\n",
        "\n",
        "for u, f in zip(urls, file_names):\n",
        "    path = mono_prefix/f\n",
        "    if not path.exists():\n",
        "        if 'mega' in u:\n",
        "            !megadl {u} --path {path}\n",
        "        else:\n",
        "            !wget {u} -O {path}\n",
        "    else:\n",
        "        print(f'{f} is exist, skip downloading')\n",
        "    if path.suffix == \".tgz\":\n",
        "        !tar -xvf {path} -C {prefix}\n",
        "    elif path.suffix == \".zip\":\n",
        "        !unzip -o {path} -d {prefix}\n",
        "    elif path.suffix == \".gz\":\n",
        "        !gzip -fkd {path}\n",
        "\n",
        "!mv {mono_prefix/'ted_zh_corpus.deduped'} {mono_prefix/'monoraw.zh'}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/DATA/DATA/rawdata/mono\n",
            "--2021-04-29 21:08:15--  https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214986&authkey=AANUKbGfZx0kM80\n",
            "Resolving onedrive.live.com (onedrive.live.com)... 13.107.42.13\n",
            "Connecting to onedrive.live.com (onedrive.live.com)|13.107.42.13|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://zla8og.dm.files.1drv.com/y4mvM5VSAwNNFUSD0Z-5ZCoYCBBEFAystX0i3H22-zJb0wJimuF8vRtjel0ixjSlS6tKqvdcwgp82s1ndIqsCUuR0uHi4UHDYhtYciTBZmK_TmZkobVq-dIXIJpvTvM8z8m-amayP6s4CLG9FH0_dMfArXnR8qDAFnuEfhs2nZWMkvPPxHTlbo5O1Mr3Wrsj-HZVIi9RN0UbW8LQok7HmwQrw/ted_zh_corpus.deduped.gz?download&psid=1 [following]\n",
            "--2021-04-29 21:08:16--  https://zla8og.dm.files.1drv.com/y4mvM5VSAwNNFUSD0Z-5ZCoYCBBEFAystX0i3H22-zJb0wJimuF8vRtjel0ixjSlS6tKqvdcwgp82s1ndIqsCUuR0uHi4UHDYhtYciTBZmK_TmZkobVq-dIXIJpvTvM8z8m-amayP6s4CLG9FH0_dMfArXnR8qDAFnuEfhs2nZWMkvPPxHTlbo5O1Mr3Wrsj-HZVIi9RN0UbW8LQok7HmwQrw/ted_zh_corpus.deduped.gz?download&psid=1\n",
            "Resolving zla8og.dm.files.1drv.com (zla8og.dm.files.1drv.com)... 13.107.43.12\n",
            "Connecting to zla8og.dm.files.1drv.com (zla8og.dm.files.1drv.com)|13.107.43.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21709855 (21M) [application/x-gzip]\n",
            "Saving to: ‘/content/DATA/DATA/rawdata/mono/ted_zh_corpus.deduped.gz’\n",
            "\n",
            "/content/DATA/DATA/ 100%[===================>]  20.70M  12.6MB/s    in 1.6s    \n",
            "\n",
            "2021-04-29 21:08:18 (12.6 MB/s) - ‘/content/DATA/DATA/rawdata/mono/ted_zh_corpus.deduped.gz’ saved [21709855/21709855]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdKl5ZO-PzTR"
      },
      "source": [
        "### TODO: 清理資料集\n",
        "\n",
        "1. 將太長、太短的句子移除\n",
        "2. 統一標點符號\n",
        "\n",
        "hint: 可以使用clean_s()來協助"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXisabHNPzTR"
      },
      "source": [
        "import re\n",
        "\n",
        "def strQ2B_mono(ustring):\n",
        "    \"\"\"把字串全形轉半形\"\"\"\n",
        "    # 參考來源:https://ithelp.ithome.com.tw/articles/10233122\n",
        "    ss = []\n",
        "    for s in ustring:\n",
        "        rstring = \"\"\n",
        "        for uchar in s:\n",
        "            inside_code = ord(uchar)\n",
        "            if inside_code == 12288:  # 全形空格直接轉換\n",
        "                inside_code = 32\n",
        "            elif (inside_code >= 65281 and inside_code <= 65374):  # 全形字元（除空格）根據關係轉化\n",
        "                inside_code -= 65248\n",
        "            rstring += chr(inside_code)\n",
        "        ss.append(rstring)\n",
        "    return ''.join(ss)\n",
        "                \n",
        "def clean_s_mono(s, lang):\n",
        "    if lang == 'zh':\n",
        "        s = strQ2B_mono(s) # Q2B\n",
        "        s = re.sub(r\"\\([^()]*\\)\", \"\", s) # remove ([text])\n",
        "        s = s.replace(' ', '')\n",
        "        s = s.replace('—', '')\n",
        "        s = s.replace('“', '\"')\n",
        "        s = s.replace('”', '\"')\n",
        "        s = s.replace('_', '')\n",
        "        s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "    s = ' '.join(s.strip().split())\n",
        "    return s\n",
        "\n",
        "def len_s_mono(s, lang):\n",
        "    if lang == 'zh':\n",
        "        return len(s)\n",
        "\n",
        "def clean_corpus_mono(prefix, l1, ratio=9, max_len=1000, min_len=1):\n",
        "    if Path(f'{prefix}.clean.{l1}').exists():\n",
        "            print(f'{prefix}.clean.{l1} exists. skipping clean.')\n",
        "            return\n",
        "    with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "      with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "          for s1 in l1_in_f:\n",
        "            s1 = s1.strip()\n",
        "            s1 = clean_s_mono(s1, l1)\n",
        "            s1_len = len_s_mono(s1, l1)\n",
        "            if min_len > 0: # remove short sentence\n",
        "                if s1_len < min_len:\n",
        "                    continue\n",
        "            if max_len > 0: # remove long sentence\n",
        "                if s1_len > max_len:\n",
        "                    continue\n",
        "                print(s1, file=l1_out_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-m4hjpoczzn4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3261ab-b43b-4903-88c0-73aa9ef69f65"
      },
      "source": [
        "monodata_prefix = f'{mono_prefix}/monoraw'\n",
        "monosrclng = 'zh'\n",
        "clean_corpus_mono(monodata_prefix, monosrclng)\n",
        "!head {monodata_prefix+'.clean.'+monosrclng} -n 5\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "在16世紀中葉意大利人被一種男歌手迷住了那種男歌手的音域廣闊 , 包含的音高先前是一般成年男性不可能達到的\n",
            "但是 , 這天賦有一個很高的代價\n",
            "要防止他們變聲這些歌手在青春期前被閹割來停止荷爾蒙的變化 , 以免他們的聲線變低沉\n",
            "被稱為 「 閹伶 」 , 他們輕輕的、天使般的聲音在整個歐洲很有名直到這個殘酷的程序 , 在19世紀被禁止\n",
            "雖然阻止聲帶的成長 , 可以產生一個非凡廣闊的音域但自然發展的聲音 , 已經具有極多的可能性\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeczZN_4PzTR"
      },
      "source": [
        "### TODO: Subword Units\n",
        "\n",
        "用反向模型的 spm model 將資料切成 subword units\n",
        "\n",
        "hint: spm model 的路徑為 DATA/raw-data/\\[dataset\\]/spm\\[vocab_num\\].model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ukqQl3BPzTR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e37cb98-4c82-4c20-9413-35f4679c7eab"
      },
      "source": [
        "vocab_size = 8000\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "monospm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "    'mono': 'monoraw.clean',\n",
        "\n",
        "}\n",
        "\n",
        "for split in ['mono']:\n",
        "    for lang in [monosrclng]:\n",
        "        out_path = mono_prefix/f'{split}.{monosrclng}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(mono_prefix/f'{split}.{monosrclng}', 'w') as out_f:\n",
        "                with open(mono_prefix/f'{in_tag[split]}.{monosrclng}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = monospm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)\n",
        "\n",
        "!head {data_dir+'/'+mono_dataset_name+'/mono.'+monosrclng} -n 5\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁在 16 世紀 中 葉 意 大 利 人 被 一種 男 歌 手 迷 住 了 那種 男 歌 手 的 音 域 廣 闊 ▁, ▁ 包 含 的 音 高 先 前 是 一般 成 年 男性 不可能 達到 的\n",
            "▁但是 ▁, ▁這 天 賦 有一個 很 高 的 代 價\n",
            "▁ 要 防 止 他們 變 聲 這些 歌 手 在 青 春 期 前 被 閹 割 來 停 止 荷 爾 蒙 的 變化 ▁, ▁以 免 他們的 聲 線 變 低 沉\n",
            "▁ 被 稱為 ▁「 ▁ 閹 伶 ▁」 ▁, ▁他們 輕 輕 的 、 天 使 般 的聲音 在 整個 歐 洲 很 有 名 直 到 這個 殘 酷 的 程 序 ▁, ▁在 19 世紀 被 禁 止\n",
            "▁雖然 阻 止 聲 帶 的 成長 ▁, ▁ 可以 產生 一個 非 凡 廣 闊 的 音 域 但 自然 發展 的聲音 ▁, ▁ 已經 具有 極 多 的 可能 性\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5nBt8pqPzTS"
      },
      "source": [
        "### Binarize\n",
        "\n",
        "使用fairseq將資料轉為binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6pYa8prPzTS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a093039a-9654-4916-f06f-24e25b592ca5"
      },
      "source": [
        "binpath = Path('./DATA/DATA/data-bin', mono_dataset_name)\n",
        "src_dict_file = './DATA/DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono\") # whatever filepath you get after applying subword tokenization\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {monopref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 21:10:13 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='DATA/DATA/data-bin/mono', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='zh', srcdict='./DATA/DATA/data-bin/ted2020/dict.en.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='./DATA/DATA/data-bin/ted2020/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/DATA/DATA/rawdata/mono/mono', user_dir=None, validpref=None, wandb_project=None, workers=2)\n",
            "2021-04-29 21:10:13 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
            "2021-04-29 21:11:23 | INFO | fairseq_cli.preprocess | [zh] /content/DATA/DATA/rawdata/mono/mono.zh: 781713 sents, 14414484 tokens, 0.00223% replaced by <unk>\n",
            "2021-04-29 21:11:23 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "2021-04-29 21:11:43 | INFO | fairseq_cli.preprocess | [en] /content/DATA/DATA/rawdata/mono/mono.en: 781713 sents, 2345139 tokens, 0.0% replaced by <unk>\n",
            "2021-04-29 21:11:43 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to DATA/DATA/data-bin/mono\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suE-NQ_iPzTS"
      },
      "source": [
        "### TODO: 生成反向翻譯資料\n",
        "\n",
        "將 binarized data 加入原本的資料夾中並用一個 split_name 取名\n",
        "\n",
        "ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "\n",
        "便可以使用 generate_prediction(model, task, split=\"split_name\")來產生翻譯資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I08ZQOfNPzTS"
      },
      "source": [
        "# 將 binarized data 加入原本的資料夾中並用一個 split_name 取名\n",
        "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "!cp ./DATA/DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "!cp ./DATA/DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "!cp ./DATA/DATA/data-bin/mono/train.zh-en.en.bin ./DATA/DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "!cp ./DATA/DATA/data-bin/mono/train.zh-en.en.idx ./DATA/DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTVq0v3SPEzX"
      },
      "source": [
        "#may load dataset here\n",
        "# !cp -r /content/drive/MyDrive/ML/HW5/DATA ./DATA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_VezEo2PzTS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117,
          "referenced_widgets": [
            "98e8872874fb45ca94fd141dc92e61ac",
            "69b7d7b9524841208903700af0d50edc",
            "37d5a209a5bf4128b43f74f5d95e105a",
            "c1451229442648a7bc9a850432645a60",
            "fd8b0f32407b41cfb830c7ce2bccdf6a",
            "bce5664005554924a01397655710c449",
            "ade5d63a1ecc4b1da14263c18124cd66",
            "075fca15dc8f456f9dc795f7b2fa5948"
          ]
        },
        "outputId": "21594088-d573-4d0d-dd88-e47e9cf13e9a"
      },
      "source": [
        "# hint: 用反向模型在 split='mono' 上進行預測，生成 prediction_file\n",
        "generate_prediction(model, task, split=\"mono\", outfile=\"./backprediction.txt\",outfilegd =\"/content/drive/MyDrive/ML/HW5/pred/backprediction.txt\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 21:12:50 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/DATA/data-bin/ted2020/mono.zh-en.zh\n",
            "2021-04-29 21:12:50 | INFO | fairseq.data.data_utils | loaded 781,713 examples from: ./DATA/DATA/data-bin/ted2020/mono.zh-en.en\n",
            "2021-04-29 21:12:50 | INFO | fairseq.tasks.translation | ./DATA/DATA/data-bin/ted2020 mono zh-en 781713 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98e8872874fb45ca94fd141dc92e61ac",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='prediction', max=3532.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chHWkbZ7PzTS"
      },
      "source": [
        "### TODO: 產生新的dataset\n",
        "\n",
        "1. 將翻譯出來的資料與原先的訓練資料結合\n",
        "2. 使用之前的spm model切出成Subword Units\n",
        "3. 重新使用fairseq將資料轉為binary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyH3U8OEPzTT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49a1d4af-2c95-43ac-b1a6-082288cc8716"
      },
      "source": [
        "# 合併剛剛生成的 prediction_file (.en) 以及中文 mono.zh (.zh)\n",
        "\n",
        "# hint: 在此用剛剛的 spm model 對 prediction_file 進行切斷詞\n",
        "back_dataset_name = \"back\"\n",
        "lng = \"txt\"\n",
        "backprefix = Path(data_dir).absolute() / back_dataset_name\n",
        "backprefix.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "in_tag = {\n",
        "    'backprediction': 'backprediction',\n",
        "\n",
        "}\n",
        "for split in ['backprediction']:\n",
        "    for lang in [lng]:\n",
        "        out_path = backprefix/f'{split}.out.{lng}'\n",
        "        if out_path.exists():\n",
        "            print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "        else:\n",
        "            with open(backprefix/f'{split}.out.{lng}', 'w') as out_f:\n",
        "                with open(backprefix/f'{in_tag[split]}.{lng}', 'r') as in_f:\n",
        "                    for line in in_f:\n",
        "                        line = line.strip()\n",
        "                        tok = monospm_model.encode(line, out_type=str)\n",
        "                        print(' '.join(tok), file=out_f)\n",
        "\n",
        "\n",
        "!head {data_dir+'/back'+'/backprediction.'+\"out.\"+lng} -n 5\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁in ▁the ▁ m id 16 th ▁century ▁, ▁it al ian s ▁were ▁f as c in ated ▁by ▁a ▁kind ▁of ▁ma le ▁ s ing er ▁who s e ▁ va s t ness ▁con tain ed ▁not es ▁pre vi ous ly ▁imp os s ible ▁for ▁a d ul t ▁men ▁.\n",
            "▁how ever ▁, ▁there ▁is ▁a ▁high ▁p ri ce ▁for ▁this ▁g if t ▁.\n",
            "▁to ▁pre v ent ▁them ▁from ▁turn ing ▁, ▁these ▁ s ing ers ▁were ▁ca s tra ted ▁before ▁ pu ber ty ▁to ▁stop ▁the ▁ho r m one ▁ sh if t ▁in ▁stop ping ▁their ▁voice s ▁from ▁fall ing ▁.\n",
            "▁known ▁as ▁ca s tra ti ▁, ▁their ▁light ▁, ▁ ang el ic ▁voice s ▁were ▁f am ous ▁through out ▁europe ▁until ▁the ▁c ru el ▁pro c ed ure ▁was ▁ba n ned ▁in ▁the ▁19 th ▁century ▁.\n",
            "▁while ▁stop ping ▁ vo cal ▁grow th ▁can ▁produce ▁an ▁ex tra or d in ary ▁ vo cal ▁ ran ge ▁, ▁natural ly ▁develop ing ▁voice s ▁already ▁have ▁en or m ous ▁po s s ib il ities ▁.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxUxOzkZ3GE5",
        "outputId": "0b109b0c-fecf-4149-a3cf-c316f9652991"
      },
      "source": [
        "#---------------------------backprediction.out.en--------------------------------------------\n",
        "\n",
        "# hint: 在此用 fairseq 把這些檔案再 binarize\n",
        "binpath = Path('./DATA/DATA/data-bin/synthetic')\n",
        "src_dict_file = './DATA/DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "backpref = './DATA/DATA/rawdata/back/backprediction.out'\n",
        "if binpath.exists():\n",
        "    print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "    !python -m fairseq_cli.preprocess\\\n",
        "        --source-lang 'zh'\\\n",
        "        --target-lang 'en'\\\n",
        "        --trainpref {backpref}\\\n",
        "        --destdir {binpath}\\\n",
        "        --srcdict {src_dict_file}\\\n",
        "        --tgtdict {tgt_dict_file}\\\n",
        "        --workers 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 23:05:05 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='DATA/DATA/data-bin/synthetic', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='zh', srcdict='./DATA/DATA/data-bin/ted2020/dict.en.txt', suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref=None, tgtdict='./DATA/DATA/data-bin/ted2020/dict.en.txt', threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='./DATA/DATA/rawdata/back/backprediction.out', user_dir=None, validpref=None, wandb_project=None, workers=2)\n",
            "2021-04-29 23:05:05 | INFO | fairseq_cli.preprocess | [zh] Dictionary: 8000 types\n",
            "2021-04-29 23:06:15 | INFO | fairseq_cli.preprocess | [zh] ./DATA/DATA/rawdata/back/backprediction.out.zh: 781713 sents, 14414484 tokens, 0.00223% replaced by <unk>\n",
            "2021-04-29 23:06:15 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "2021-04-29 23:07:30 | INFO | fairseq_cli.preprocess | [en] ./DATA/DATA/rawdata/back/backprediction.out.en: 781713 sents, 17669210 tokens, 0.0% replaced by <unk>\n",
            "2021-04-29 23:07:30 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to DATA/DATA/data-bin/synthetic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNQOte8dPzTT"
      },
      "source": [
        "# 這裡用剛剛準備的檔案合併原先 ted2020 來生成最終 back-translation 的資料\n",
        "!cp -r ./DATA/DATA/data-bin/ted2020/ ./DATA/DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "!cp ./DATA/DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "!cp ./DATA/DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "!cp ./DATA/DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "!cp ./DATA/DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sImfdzBHPzTT"
      },
      "source": [
        "### TODO: 重新訓練\n",
        "\n",
        "當已經產生新的資料集\n",
        "\n",
        "1. 將實驗的參數設定表(config)中的datadir改為新的資料集(\"./DATA/data-bin/ted2020_with_mono\")\n",
        "2. 將實驗的參數設定表(config)中的source_lang與target_lang設定還原(\"en\", \"zh\")\n",
        "3. 將實驗的參數設定表(config)中的savedir更改(ex. \"./checkpoints/rnn-bt\")\n",
        "4. 重新訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juazXP2SPzTT"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JivkkF_BPzTT"
      },
      "source": [
        "1. <a name=ott2019fairseq></a>Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., ... & Auli, M. (2019, June). fairseq: A Fast, Extensible Toolkit for Sequence Modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations) (pp. 48-53).\n",
        "2. <a name=vaswani2017></a>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017, December). Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (pp. 6000-6010).\n",
        "3. <a name=reimers-2020-multilingual-sentence-bert></a>Reimers, N., & Gurevych, I. (2020, November). Making Monolingual Sentence Embeddings Multilingual Using Knowledge Distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) (pp. 4512-4525).\n",
        "4. <a name=tiedemann2012parallel></a>Tiedemann, J. (2012, May). Parallel Data, Tools and Interfaces in OPUS. In Lrec (Vol. 2012, pp. 2214-2218).\n",
        "5. <a name=kudo-richardson-2018-sentencepiece></a>Kudo, T., & Richardson, J. (2018, November). SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 66-71).\n",
        "6. <a name=sennrich-etal-2016-improving></a>Sennrich, R., Haddow, B., & Birch, A. (2016, August). Improving Neural Machine Translation Models with Monolingual Data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) (pp. 86-96).\n",
        "7. <a name=edunov-etal-2018-understanding></a>Edunov, S., Ott, M., Auli, M., & Grangier, D. (2018). Understanding Back-Translation at Scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (pp. 489-500).\n",
        "8. https://github.com/ajinkyakulkarni14/TED-Multilingual-Parallel-Corpus\n",
        "9. https://ithelp.ithome.com.tw/articles/10233122\n",
        "10. https://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    }
  ]
}