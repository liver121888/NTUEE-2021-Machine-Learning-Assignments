{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"「HW04.ipynb」的副本","provenance":[{"file_id":"https://github.com/ga642381/ML2021-Spring/blob/main/HW04/HW04.ipynb","timestamp":1617431357391}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python385jvsc74a57bd046daca7bde7b2f7630605fc986966a9bc94e02b3aafa19d55f96b74733d1517b","display_name":"Python 3.8.5 64-bit ('pytorch_gpu_v3': conda)"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5d8b2cf8cd9b45e895db75a3c03ea50f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ac24cd21a98c48a6833a3e20408880f1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a84525fe14394f8fbe0b5b431fbf8f40","IPY_MODEL_c609cb1c10984d7fa41bf2dfca5f7491"]}},"ac24cd21a98c48a6833a3e20408880f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a84525fe14394f8fbe0b5b431fbf8f40":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2c1206b3f0cc4b56aa1d655b91f163f0","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":6000,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6000,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3464a54025242df8c2b2eaf6a4f0f23"}},"c609cb1c10984d7fa41bf2dfca5f7491":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_af5c3c54648642d9bcb7f55c53a2a473","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 6000/6000 [00:22&lt;00:00, 261.39it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_388e2b89e65f4e2badc528a848b7ce3c"}},"2c1206b3f0cc4b56aa1d655b91f163f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f3464a54025242df8c2b2eaf6a4f0f23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af5c3c54648642d9bcb7f55c53a2a473":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"388e2b89e65f4e2badc528a848b7ce3c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"metadata":{"interpreter":{"hash":"57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"}}},"cells":[{"cell_type":"markdown","metadata":{"id":"zC5KwRyl6Flp"},"source":["# Task description\n","- Classify the speakers of given features.\n","- Main goal: Learn how to use transformer.\n","- Baselines:\n","  - Easy: Run sample code and know how to use transformer.\n","  - Medium: Know how to adjust parameters of transformer.\n","  - Hard: Construct [conformer](https://arxiv.org/abs/2005.08100) which is a variety of transformer. \n","\n","- Other links\n","  - Kaggle: [link](https://www.kaggle.com/t/859c9ca9ede14fdea841be627c412322)\n","  - Slide: [link](https://speech.ee.ntu.edu.tw/~hylee/ml/ml2021-course-data/hw/HW04/HW04.pdf)\n","  - Data: [link](https://drive.google.com/file/d/1T0RPnu-Sg5eIPwQPfYysipfcz81MnsYe/view?usp=sharing)\n","  - Video (Chinese): [link](https://www.youtube.com/watch?v=EPerg2UnGaI)\n","  - Video (English): [link](https://www.youtube.com/watch?v=Gpz6AUvCak0)\n","  - Solution for downloading dataset fail.: [link](https://drive.google.com/drive/folders/13T0Pa_WGgQxNkqZk781qhc5T9-zfh19e?usp=sharing)"]},{"cell_type":"markdown","metadata":{"id":"Mz_NpuAipk3h"},"source":["## Dataset\n","- Original dataset is [Voxceleb1](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/).\n","- The [license](https://creativecommons.org/licenses/by/4.0/) and [complete version](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/license.txt) of Voxceleb1.\n","- We randomly select 600 speakers from Voxceleb1.\n","- Then preprocess the raw waveforms into mel-spectrograms.\n","\n","- Args:\n","  - data_dir: The path to the data directory.\n","  - metadata_path: The path to the metadata.\n","  - segment_len: The length of audio segment for training. \n","- The architecture of data directory \\\\\n","  - data directory \\\\\n","  |---- metadata.json \\\\\n","  |---- testdata.json \\\\\n","  |---- mapping.json \\\\\n","  |---- uttr-{random string}.pt \\\\\n","\n","- The information in metadata\n","  - \"n_mels\": The dimention of mel-spectrogram.\n","  - \"speakers\": A dictionary. \n","    - Key: speaker ids.\n","    - value: \"feature_path\" and \"mel_len\"\n","\n","\n","For efficiency, we segment the mel-spectrograms into segments in the traing step."]},{"cell_type":"code","metadata":{"id":"cd7hoGhYtbXQ","executionInfo":{"status":"ok","timestamp":1618231113123,"user_tz":-480,"elapsed":44,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}}},"source":["import os\n","import json\n","import torch\n","import random\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence\n"," \n"," \n","class myDataset(Dataset):\n","  def __init__(self, data_dir, segment_len=128):\n","    self.data_dir = data_dir\n","    self.segment_len = segment_len\n"," \n","    # Load the mapping from speaker neme to their corresponding id. \n","    mapping_path = Path(data_dir) / \"mapping.json\"\n","    mapping = json.load(mapping_path.open())\n","    self.speaker2id = mapping[\"speaker2id\"]\n"," \n","    # Load metadata of training data.\n","    metadata_path = Path(data_dir) / \"metadata.json\"\n","    metadata = json.load(open(metadata_path))[\"speakers\"]\n"," \n","    # Get the total number of speaker.\n","    self.speaker_num = len(metadata.keys())\n","    self.data = []\n","    for speaker in metadata.keys():\n","      for utterances in metadata[speaker]:\n","        self.data.append([utterances[\"feature_path\"], self.speaker2id[speaker]])\n"," \n","  def __len__(self):\n","    return len(self.data)\n"," \n","  def __getitem__(self, index):\n","    feat_path, speaker = self.data[index]\n","    # Load preprocessed mel-spectrogram.\n","    mel = torch.load(os.path.join(self.data_dir, feat_path))\n"," \n","    # Segmemt mel-spectrogram into \"segment_len\" frames.\n","    if len(mel) > self.segment_len:\n","      # Randomly get the starting point of the segment.\n","      start = random.randint(0, len(mel) - self.segment_len)\n","      # Get a segment with \"segment_len\" frames.\n","      mel = torch.FloatTensor(mel[start:start+self.segment_len])\n","    else:\n","      mel = torch.FloatTensor(mel)\n","    # Turn the speaker id into long for computing loss later.\n","    speaker = torch.FloatTensor([speaker]).long()\n","    return mel, speaker\n"," \n","  def get_speaker_number(self):\n","    return self.speaker_num"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqJxjoi_NGnB"},"source":["## Dataloader\n","- Split dataset into training dataset(90%) and validation dataset(10%).\n","- Create dataloader to iterate the data.\n"]},{"cell_type":"code","metadata":{"id":"zuT1AuFENI8t","executionInfo":{"status":"ok","timestamp":1618231113123,"user_tz":-480,"elapsed":38,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}}},"source":["import torch\n","from torch.utils.data import DataLoader, random_split\n","from torch.nn.utils.rnn import pad_sequence\n","\n","\n","def collate_batch(batch):\n","  # Process features within a batch.\n","  \"\"\"Collate a batch of data.\"\"\"\n","  mel, speaker = zip(*batch)\n","  # Because we train the model batch by batch, we need to pad the features in the same batch to make their lengths the same.\n","  mel = pad_sequence(mel, batch_first=True, padding_value=-20)    # pad log 10^(-20) which is very small value.\n","  # mel: (batch size, length, 40)\n","  return mel, torch.FloatTensor(speaker).long()\n","\n","\n","def get_dataloader(data_dir, batch_size, n_workers):\n","  \"\"\"Generate dataloader\"\"\"\n","  dataset = myDataset(data_dir)\n","  speaker_num = dataset.get_speaker_number()\n","  # Split dataset into training dataset and validation dataset\n","  trainlen = int(0.9 * len(dataset))\n","  lengths = [trainlen, len(dataset) - trainlen]\n","  trainset, validset = random_split(dataset, lengths)\n","\n","  train_loader = DataLoader(\n","    trainset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    drop_last=True,\n","    num_workers=n_workers,\n","    pin_memory=True,\n","    collate_fn=collate_batch,\n","  )\n","  valid_loader = DataLoader(\n","    validset,\n","    batch_size=batch_size,\n","    num_workers=n_workers,\n","    drop_last=True,\n","    pin_memory=True,\n","    collate_fn=collate_batch,\n","  )\n","\n","  return train_loader, valid_loader, speaker_num\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"SHX4eVj4tjtd","executionInfo":{"status":"ok","timestamp":1618231460194,"user_tz":-480,"elapsed":7020,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import Tensor\n","from typing import Tuple\n","\n","# conformersorce: https://github.com/sooftware/conformer\n","from conformer import Conformer\n","from conformer.feed_forward import FeedForwardModule\n","from conformer.attention import MultiHeadedSelfAttentionModule\n","from conformer.convolution import (\n","    ConformerConvModule,\n","    Conv2dSubampling,\n",")\n","from conformer.modules import (\n","    ResidualConnectionModule,\n","    LayerNorm,\n","    Linear,\n",")\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","a = torch.full((32, 1), 128)\n","\n","import torch\n","import torch.nn as nn\n","from torch import Tensor\n","\n","from conformer.decoder import DecoderRNNT\n","from conformer.encoder import ConformerEncoder\n","from conformer.modules import Linear\n","\n","\n","\n","\n","class Conv2dSubampling(nn.Module):\n","    \"\"\"\n","    Convolutional 2D subsampling (to 1/4 length)\n","\n","    Args:\n","        in_channels (int): Number of channels in the input image\n","        out_channels (int): Number of channels produced by the convolution\n","\n","    Inputs: inputs\n","        - **inputs** (batch, time, dim): Tensor containing sequence of inputs\n","\n","    Returns: outputs, output_lengths\n","        - **outputs** (batch, time, dim): Tensor produced by the convolution\n","        - **output_lengths** (batch): list of sequence output lengths\n","    \"\"\"\n","    def __init__(self, in_channels: int, out_channels: int) -> None:\n","        super(Conv2dSubampling, self).__init__()\n","        self.sequential = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2),\n","            nn.ReLU(),\n","        )\n","\n","    def forward(self, inputs: Tensor, input_lengths: Tensor) -> Tuple[Tensor, Tensor]:\n","        outputs = inputs\n","\n","\n","        output_lengths = a\n","\n","        return outputs, output_lengths\n","\n","\n","\n","\n","class ConformerBlock(nn.Module):\n","    \"\"\"\n","    Conformer block contains two Feed Forward modules sandwiching the Multi-Headed Self-Attention module\n","    and the Convolution module. This sandwich structure is inspired by Macaron-Net, which proposes replacing\n","    the original feed-forward layer in the Transformer block into two half-step feed-forward layers,\n","    one before the attention layer and one after.\n","\n","    Args:\n","        encoder_dim (int, optional): Dimension of conformer encoder\n","        num_attention_heads (int, optional): Number of attention heads\n","        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n","        conv_expansion_factor (int, optional): Expansion factor of conformer convolution module\n","        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n","        attention_dropout_p (float, optional): Probability of attention module dropout\n","        conv_dropout_p (float, optional): Probability of conformer convolution module dropout\n","        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n","        half_step_residual (bool): Flag indication whether to use half step residual or not\n","        device (torch.device): torch device (cuda or cpu)\n","\n","    Inputs: inputs\n","        - **inputs** (batch, time, dim): Tensor containing input vector\n","\n","    Returns: outputs\n","        - **outputs** (batch, time, dim): Tensor produces by conformer block.\n","    \"\"\"\n","    def __init__(\n","            self,\n","            encoder_dim: int = 512,\n","            num_attention_heads: int = 8,\n","            feed_forward_expansion_factor: int = 4,\n","            conv_expansion_factor: int = 2,\n","            feed_forward_dropout_p: float = 0.1,\n","            attention_dropout_p: float = 0.1,\n","            conv_dropout_p: float = 0.1,\n","            conv_kernel_size: int = 31,\n","            half_step_residual: bool = True,\n","            device: torch.device = 'cuda',\n","    ):\n","        super(ConformerBlock, self).__init__()\n","        self.device = device\n","        if half_step_residual:\n","            self.feed_forward_residual_factor = 0.5\n","        else:\n","            self.feed_forward_residual_factor = 1\n","\n","        self.sequential = nn.Sequential(\n","            ResidualConnectionModule(\n","                module=FeedForwardModule(\n","                    encoder_dim=encoder_dim,\n","                    expansion_factor=feed_forward_expansion_factor,\n","                    dropout_p=feed_forward_dropout_p,\n","                    device=device,\n","                ),\n","                module_factor=self.feed_forward_residual_factor,\n","            ),\n","            ResidualConnectionModule(\n","                module=MultiHeadedSelfAttentionModule(\n","                    d_model=encoder_dim,\n","                    num_heads=num_attention_heads,\n","                    dropout_p=attention_dropout_p,\n","                ),\n","            ),\n","            ResidualConnectionModule(\n","                module=ConformerConvModule(\n","                    in_channels=encoder_dim,\n","                    kernel_size=conv_kernel_size,\n","                    expansion_factor=conv_expansion_factor,\n","                    dropout_p=conv_dropout_p,\n","                ),\n","            ),\n","            ResidualConnectionModule(\n","                module=FeedForwardModule(\n","                    encoder_dim=encoder_dim,\n","                    expansion_factor=feed_forward_expansion_factor,\n","                    dropout_p=feed_forward_dropout_p,\n","                ),\n","                module_factor=self.feed_forward_residual_factor,\n","            ),\n","            LayerNorm(encoder_dim),\n","        )\n","\n","    def forward(self, inputs: Tensor) -> Tensor:\n","        return self.sequential(inputs.to(self.device))\n","\n","\n","class ConformerEncoder(nn.Module):\n","    \"\"\"\n","    Conformer encoder first processes the input with a convolution subsampling layer and then\n","    with a number of conformer blocks.\n","\n","    Args:\n","        input_dim (int, optional): Dimension of input vector\n","        encoder_dim (int, optional): Dimension of conformer encoder\n","        num_layers (int, optional): Number of conformer blocks\n","        num_attention_heads (int, optional): Number of attention heads\n","        feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n","        conv_expansion_factor (int, optional): Expansion factor of conformer convolution module\n","        feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n","        attention_dropout_p (float, optional): Probability of attention module dropout\n","        conv_dropout_p (float, optional): Probability of conformer convolution module dropout\n","        conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n","        half_step_residual (bool): Flag indication whether to use half step residual or not\n","        device (torch.device): torch device (cuda or cpu)\n","\n","    Inputs: inputs, input_lengths\n","        - **inputs** (batch, time, dim): Tensor containing input vector\n","        - **input_lengths** (batch): list of sequence input lengths\n","\n","    Returns: outputs, output_lengths\n","        - **outputs** (batch, out_channels, time): Tensor produces by conformer encoder.\n","        - **output_lengths** (batch): list of sequence output lengths\n","    \"\"\"\n","    def __init__(\n","            self,\n","            input_dim: int = 80,\n","            encoder_dim: int = 512,\n","            num_layers: int = 17,\n","            num_attention_heads: int = 8,\n","            feed_forward_expansion_factor: int = 4,\n","            conv_expansion_factor: int = 2,\n","            input_dropout_p: float = 0.1,\n","            feed_forward_dropout_p: float = 0.1,\n","            attention_dropout_p: float = 0.1,\n","            conv_dropout_p: float = 0.1,\n","            conv_kernel_size: int = 31,\n","            half_step_residual: bool = True,\n","            device: torch.device = 'cuda',\n","    ):\n","        super(ConformerEncoder, self).__init__()\n","        self.conv_subsample = Conv2dSubampling(in_channels=1, out_channels=encoder_dim)\n","        self.input_projection = nn.Sequential(\n","            Linear(encoder_dim * (((input_dim - 1) // 2 - 1) // 2), encoder_dim),\n","            nn.Dropout(p=input_dropout_p),\n","        )\n","        self.layers = nn.ModuleList([ConformerBlock(\n","            encoder_dim=encoder_dim,\n","            num_attention_heads=num_attention_heads,\n","            feed_forward_expansion_factor=feed_forward_expansion_factor,\n","            conv_expansion_factor=conv_expansion_factor,\n","            feed_forward_dropout_p=feed_forward_dropout_p,\n","            attention_dropout_p=attention_dropout_p,\n","            conv_dropout_p=conv_dropout_p,\n","            conv_kernel_size=conv_kernel_size,\n","            half_step_residual=half_step_residual,\n","            device=device,\n","        ).to(device) for _ in range(num_layers)])\n","\n","    def count_parameters(self) -> int:\n","        \"\"\" Count parameters of encoder \"\"\"\n","        return sum([p.numel for p in self.parameters()])\n","\n","    def update_dropout(self, dropout_p: float) -> None:\n","        \"\"\" Update dropout probability of encoder \"\"\"\n","        for name, child in self.named_children():\n","            if isinstance(child, nn.Dropout):\n","                child.p = dropout_p\n","\n","    def forward(self, inputs: Tensor, input_lengths = a ) -> Tuple[Tensor, Tensor]:\n","        \"\"\"\n","        Forward propagate a `inputs` for  encoder training.\n","\n","        Args:\n","            inputs (torch.FloatTensor): A input sequence passed to encoder. Typically for inputs this will be a padded\n","                `FloatTensor` of size ``(batch, seq_length, dimension)``. # [32 128 40]\n","            input_lengths (torch.LongTensor): The length of input tensor. ``(batch)`` [1 32]\n","\n","        Returns:\n","            (Tensor, Tensor)\n","\n","            * outputs (torch.FloatTensor): A output sequence of encoder. `FloatTensor` of size\n","                ``(batch, seq_length, dimension)`` #32 31 32\n","            * output_lengths (torch.LongTensor): The length of output tensor. ``(batch)``\n","        \"\"\"\n","        outputs, output_lengths = self.conv_subsample(inputs, input_lengths)\n","        # print(outputs.shape)\n","        # outputs = self.input_projection(outputs)\n","        # outputs = outputs.unsqueeze(1)\n","        # print(2)\n","        # print(outputs.shape)\n","\n","\n","        for layer in self.layers:\n","            outputs = layer(outputs)\n","\n","        # print(outputs.shape)\n","        return outputs, output_lengths\n","\n","\n","class Classifier(nn.Module):\n","  def __init__(self, d_model=40, n_spks=600, dropout=0.3): #d_model small d_model=80, n_spks=600, dropout=0.1\n","    super().__init__()\n","    # Project the dimension of features from that of input into d_model.\n","    # self.prenet = nn.Linear(40, d_model)\n","    #   Change Transformer to Conformer.\n","    #   https://arxiv.org/abs/2005.08100\n","    \n","    \n","    \n","    # self.encoder_layer = nn.TransformerEncoderLayer(\n","    #   d_model=d_model, dim_feedforward=256, nhead=1\n","    # )\n","    # self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=2) #多層transformer\n","\n","\n","    self.ConformerEncoderLayer = ConformerEncoder(input_dim=40, encoder_dim = 40,\n","            num_layers = 3,num_attention_heads = 1,\n","            input_dropout_p = 0.1,\n","            feed_forward_dropout_p = 0.1,\n","            attention_dropout_p = 0.1,\n","            conv_dropout_p = 0.1,\n",")\n","    \n","    # Args:\n","    #     num_classes (int): Number of classification classes\n","    #     input_dim (int, optional): Dimension of input vector\n","    #     encoder_dim (int, optional): Dimension of conformer encoder\n","    #     decoder_dim (int, optional): Dimension of conformer decoder\n","    #     num_encoder_layers (int, optional): Number of conformer blocks\n","    #     num_decoder_layers (int, optional): Number of decoder layers\n","    #     decoder_rnn_type (str, optional): type of RNN cell\n","    #     num_attention_heads (int, optional): Number of attention heads\n","    #     feed_forward_expansion_factor (int, optional): Expansion factor of feed forward module\n","    #     conv_expansion_factor (int, optional): Expansion factor of conformer convolution module\n","    #     feed_forward_dropout_p (float, optional): Probability of feed forward module dropout\n","    #     attention_dropout_p (float, optional): Probability of attention module dropout\n","    #     conv_dropout_p (float, optional): Probability of conformer convolution module dropout\n","    #     decoder_dropout_p (float, optional): Probability of conformer decoder dropout\n","    #     conv_kernel_size (int or tuple, optional): Size of the convolving kernel\n","    #     half_step_residual (bool): Flag indication whether to use half step residual or not\n","    #     device (torch.device): torch device (cuda or cpu)\n","    # Inputs: inputs\n","    #     - **inputs** (batch, time, dim): Tensor containing input vector\n","    #     - **input_lengths** (batch): list of sequence input lengths\n","    # Returns: outputs, output_lengths\n","    #     - **outputs** (batch, out_channels, time): Tensor produces by conformer.\n","    #     - **output_lengths** (batch): list of sequence output lengths\n","    \n","\n","    # self.con = Conformer(num_classes=600, input_dim=d_model, \n","    #                               encoder_dim=d_model, num_encoder_layers=3, \n","    #                               decoder_dim=32, device=device)\n","\n","\n","\n","    # Project the the dimension of features from d_model into speaker nums.\n","    self.pred_layer = nn.Sequential(\n","      nn.Linear(d_model, d_model), #maybe comment\n","      nn.ReLU(),\n","      nn.Linear(d_model, n_spks),\n","    )\n","\n","  def forward(self, mels):\n","    \"\"\"\n","    args:\n","      mels: (batch size, length, 40) [32 128 40]\n","    return:\n","      out: (batch size, n_spks)\n","    \"\"\"\n","\n","    # # out: (batch size, length, d_model)\n","    # out = self.prenet(mels)\n","    # print(out)\n","    # print(out.shape)\n","    # # out: (length, batch size, d_model)\n","    # out = out.permute(0, 1, 2)\n","    # print(out)\n","    # print(out.shape)\n","    \n","    # print(out.shape)\n","    # print(type(out))\n","    # #([32, 128, 40])\n","    # # The encoder layer expect features in the shape of (length, batch size, d_model).\n","    # print(mels.shape)\n","    out = self.ConformerEncoderLayer(mels, a)\n","    # print(out[0])\n","    # print(out[0].shape)\n","    out = out[0]\n","    # print(out.shape) #torch.Size(32, 128, 40])\n","    # print(type(out))\n","    # print(4)\n","\n","    \n","    # out = out.transpose(0, 1)\n","    # print(out.shape)\n","\n","    # mean pooling need 32 128 40\n","    stats = out.mean(dim=1)\n","    # print(stats.shape)\n","    # out: (batch, n_spks)\n","    out = self.pred_layer(stats)\n","    return out\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-__DolPGpvDZ"},"source":["# Learning rate schedule\n","- For transformer architecture, the design of learning rate schedule is different from that of CNN.\n","- Previous works show that the warmup of learning rate is useful for training models with transformer architectures.\n","- The warmup schedule\n","  - Set learning rate to 0 in the beginning.\n","  - The learning rate increases linearly from 0 to initial learning rate during warmup period."]},{"cell_type":"code","metadata":{"id":"K-0816BntqT9","executionInfo":{"status":"ok","timestamp":1618231466764,"user_tz":-480,"elapsed":1681,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}}},"source":["import math\n","\n","import torch\n","from torch.optim import Optimizer\n","from torch.optim.lr_scheduler import LambdaLR\n","\n","\n","def get_cosine_schedule_with_warmup(\n","  optimizer: Optimizer,\n","  num_warmup_steps: int,\n","  num_training_steps: int,\n","  num_cycles: float = 0.5,\n","  last_epoch: int = -1,\n","):\n","  \"\"\"\n","  Create a schedule with a learning rate that decreases following the values of the cosine function between the\n","  initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the\n","  initial lr set in the optimizer.\n","\n","  Args:\n","    optimizer (:class:`~torch.optim.Optimizer`):\n","      The optimizer for which to schedule the learning rate.\n","    num_warmup_steps (:obj:`int`):\n","      The number of steps for the warmup phase.\n","    num_training_steps (:obj:`int`):\n","      The total number of training steps.\n","    num_cycles (:obj:`float`, `optional`, defaults to 0.5):\n","      The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0\n","      following a half-cosine).\n","    last_epoch (:obj:`int`, `optional`, defaults to -1):\n","      The index of the last epoch when resuming training.\n","\n","  Return:\n","    :obj:`torch.optim.lr_scheduler.LambdaLR` with the appropriate schedule.\n","  \"\"\"\n","\n","  def lr_lambda(current_step):\n","    # Warmup\n","    if current_step < num_warmup_steps:\n","      return float(current_step) / float(max(1, num_warmup_steps))\n","    # decadence\n","    progress = float(current_step - num_warmup_steps) / float(\n","      max(1, num_training_steps - num_warmup_steps)\n","    )\n","    return max(\n","      0.0, 0.5 * (1.0 + math.cos(math.pi * float(num_cycles) * 2.0 * progress))\n","    )\n","\n","  return LambdaLR(optimizer, lr_lambda, last_epoch)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IP03FFo9K8DS"},"source":["# Model Function\n","- Model forward function."]},{"cell_type":"code","metadata":{"id":"fohaLEFJK9-t","executionInfo":{"status":"ok","timestamp":1618231466764,"user_tz":-480,"elapsed":1673,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}}},"source":["import torch\n","\n","\n","def model_fn(batch, model, criterion, device):\n","  \"\"\"Forward a batch through the model.\"\"\"\n","\n","  mels, labels = batch\n","  mels = mels.to(device)\n","  labels = labels.to(device)\n","\n","  outs = model(mels)\n","\n","  loss = criterion(outs, labels)\n","\n","  # Get the speaker id with highest probability.\n","  preds = outs.argmax(1)\n","  # Compute accuracy.\n","  accuracy = torch.mean((preds == labels).float())\n","\n","  return loss, accuracy\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F7cg-YrzLQcf"},"source":["# Validate\n","- Calculate accuracy of the validation set."]},{"cell_type":"code","metadata":{"id":"mD-_p6nWLO2L","executionInfo":{"status":"ok","timestamp":1618231466765,"user_tz":-480,"elapsed":1671,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}}},"source":["from tqdm import tqdm\n","import torch\n","\n","\n","def valid(dataloader, model, criterion, device): \n","  \"\"\"Validate on validation set.\"\"\"\n","\n","  model.eval()\n","  running_loss = 0.0\n","  running_accuracy = 0.0\n","  pbar = tqdm(total=len(dataloader.dataset), ncols=0, desc=\"Valid\", unit=\" uttr\")\n","\n","  for i, batch in enumerate(dataloader):\n","    with torch.no_grad():\n","      loss, accuracy = model_fn(batch, model, criterion, device)\n","      running_loss += loss.item()\n","      running_accuracy += accuracy.item()\n","\n","    pbar.update(dataloader.batch_size)\n","    pbar.set_postfix(\n","      loss=f\"{running_loss / (i+1):.2f}\",\n","      accuracy=f\"{running_accuracy / (i+1):.2f}\",\n","    )\n","\n","  pbar.close()\n","  model.train()\n","\n","  return running_accuracy / len(dataloader)\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"noHXyal5p1W5"},"source":["# Main function"]},{"cell_type":"code","metadata":{"id":"chRQE7oYtw62","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618239751973,"user_tz":-480,"elapsed":3138268,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}},"outputId":"3ff4cd7e-5a46-4e74-9592-d012c404fb20","tags":[]},"source":["from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, random_split\n","\n","# fix random seed\n","def same_seeds(seed):\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  \n","    np.random.seed(seed)  \n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","def parse_args():\n","  \"\"\"arguments\"\"\"\n","  config = {\n","    \"data_dir\": \"C:/Users/david/ML/HW4/Dataset\",\n","    \"save_path\": \"C:/Users/david/ML/HW4/modelmodi.ckpt\",\n","    \"batch_size\": 32, #32\n","    \"n_workers\": 0, #8\n","    \"valid_steps\": 2000, #2000\n","    \"warmup_steps\": 1000, #1000\n","    \"save_steps\": 10000, #10000\n","    \"total_steps\": 2000000, #70000\n","  }\n","\n","  return config\n","\n","\n","def main(\n","  data_dir,\n","  save_path,\n","  batch_size,\n","  n_workers,\n","  valid_steps,\n","  warmup_steps,\n","  total_steps,\n","  save_steps,\n","):\n","  \"\"\"Main function.\"\"\"\n","  #fix seed\n","  same_seeds(0)\n","\n","\n","  \n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  print(f\"[Info]: Use {device} now!\")\n","\n","  train_loader, valid_loader, speaker_num = get_dataloader(data_dir, batch_size, n_workers)\n","  train_iterator = iter(train_loader)\n","  print(f\"[Info]: Finish loading data!\",flush = True)\n","\n","  model = Classifier(n_spks=speaker_num).to(device)\n","\n","  # may load checkpoint\n","  model.load_state_dict(torch.load(save_path))\n","\n","\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = AdamW(model.parameters(), lr=1e-3)\n","  scheduler = get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n","  print(f\"[Info]: Finish creating model!\",flush = True)\n","\n","  best_accuracy = -1.0\n","  best_state_dict = None\n","\n","  pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","  for step in range(total_steps):\n","    # Get data\n","    try:\n","      batch = next(train_iterator)\n","    except StopIteration:\n","      train_iterator = iter(train_loader)\n","      batch = next(train_iterator)\n","\n","    loss, accuracy = model_fn(batch, model, criterion, device)\n","    batch_loss = loss.item()\n","    batch_accuracy = accuracy.item()\n","\n","    # Updata model\n","    loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","    optimizer.zero_grad()\n","    \n","    # Log\n","    pbar.update()\n","    pbar.set_postfix(\n","      loss=f\"{batch_loss:.2f}\",\n","      accuracy=f\"{batch_accuracy:.2f}\",\n","      step=step + 1,\n","    )\n","\n","    # Do validation\n","    if (step + 1) % valid_steps == 0:\n","      pbar.close()\n","\n","      valid_accuracy = valid(valid_loader, model, criterion, device)\n","\n","      # keep the best model\n","      if valid_accuracy > best_accuracy:\n","        best_accuracy = valid_accuracy\n","        best_state_dict = model.state_dict()\n","\n","      pbar = tqdm(total=valid_steps, ncols=0, desc=\"Train\", unit=\" step\")\n","\n","    # Save the best model so far.\n","    if (step + 1) % save_steps == 0 and best_state_dict is not None:\n","      torch.save(best_state_dict, save_path)\n","      pbar.write(f\"Step {step + 1}, best model saved. (accuracy={best_accuracy:.4f})\")\n","\n","  pbar.close()\n","\n","\n","if __name__ == \"__main__\":\n","  main(**parse_args())\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info]: Use cuda now!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n","Train: 100% 2000/2000 [04:06<00:00,  8.10 step/s, accuracy=0.78, loss=1.03, step=2000]\n","Valid: 100% 6944/6944 [00:04<00:00, 1568.14 uttr/s, accuracy=0.76, loss=1.02]\n","Train: 100% 2000/2000 [04:04<00:00,  8.17 step/s, accuracy=0.78, loss=1.29, step=4000]\n","Valid: 100% 6944/6944 [00:39<00:00, 174.89 uttr/s, accuracy=0.75, loss=1.05]\n","Train: 100% 2000/2000 [04:18<00:00,  7.73 step/s, accuracy=0.66, loss=1.52, step=6000]\n","Valid: 100% 6944/6944 [00:32<00:00, 213.79 uttr/s, accuracy=0.76, loss=1.00]\n","Train: 100% 2000/2000 [04:11<00:00,  7.96 step/s, accuracy=0.81, loss=0.69, step=8000]\n","Valid: 100% 6944/6944 [00:29<00:00, 231.65 uttr/s, accuracy=0.73, loss=1.05]\n","Train: 100% 2000/2000 [04:13<00:00,  7.90 step/s, accuracy=0.81, loss=0.66, step=1e+4]\n","Valid: 100% 6944/6944 [00:27<00:00, 250.75 uttr/s, accuracy=0.75, loss=1.06]\n","Train:   0% 1/2000 [00:00<05:18,  6.27 step/s, accuracy=0.75, loss=0.96, step=1e+4]Step 10000, best model saved. (accuracy=0.7573)\n","Train: 100% 2000/2000 [03:45<00:00,  8.88 step/s, accuracy=0.69, loss=1.02, step=12000]\n","Valid: 100% 6944/6944 [00:36<00:00, 191.77 uttr/s, accuracy=0.75, loss=1.02]\n","Train: 100% 2000/2000 [03:20<00:00, 10.00 step/s, accuracy=0.88, loss=0.60, step=14000]\n","Valid: 100% 6944/6944 [00:26<00:00, 258.43 uttr/s, accuracy=0.75, loss=1.04]\n","Train: 100% 2000/2000 [03:17<00:00, 10.11 step/s, accuracy=0.75, loss=0.84, step=16000]\n","Valid: 100% 6944/6944 [00:27<00:00, 249.53 uttr/s, accuracy=0.75, loss=1.05]\n","Train: 100% 2000/2000 [02:03<00:00, 16.20 step/s, accuracy=0.72, loss=0.98, step=18000]\n","Valid: 100% 6944/6944 [00:05<00:00, 1248.67 uttr/s, accuracy=0.75, loss=1.03]\n","Train: 100% 2000/2000 [01:56<00:00, 17.20 step/s, accuracy=0.78, loss=0.81, step=2e+4]\n","Valid: 100% 6944/6944 [00:09<00:00, 728.91 uttr/s, accuracy=0.74, loss=1.05]\n","Train:   0% 3/2000 [00:00<02:26, 13.64 step/s, accuracy=0.84, loss=0.66, step=2e+4]Step 20000, best model saved. (accuracy=0.7573)\n","Train: 100% 2000/2000 [01:47<00:00, 18.56 step/s, accuracy=0.81, loss=0.96, step=22000]\n","Valid: 100% 6944/6944 [00:04<00:00, 1505.74 uttr/s, accuracy=0.74, loss=1.01]\n","Train: 100% 2000/2000 [01:41<00:00, 19.66 step/s, accuracy=0.75, loss=0.78, step=24000]\n","Valid: 100% 6944/6944 [00:04<00:00, 1531.57 uttr/s, accuracy=0.74, loss=1.07]\n","Train: 100% 2000/2000 [01:41<00:00, 19.64 step/s, accuracy=0.69, loss=1.25, step=26000]\n","Valid: 100% 6944/6944 [00:04<00:00, 1524.36 uttr/s, accuracy=0.74, loss=1.07]\n","Train: 100% 2000/2000 [01:52<00:00, 17.85 step/s, accuracy=0.81, loss=0.65, step=28000]\n","Valid: 100% 6944/6944 [00:09<00:00, 746.09 uttr/s, accuracy=0.75, loss=1.03]\n","Train: 100% 2000/2000 [01:49<00:00, 18.29 step/s, accuracy=0.78, loss=0.68, step=3e+4]\n","Valid: 100% 6944/6944 [00:09<00:00, 744.98 uttr/s, accuracy=0.75, loss=1.01]\n","Train:   0% 3/2000 [00:00<02:25, 13.74 step/s, accuracy=0.69, loss=0.94, step=3e+4]Step 30000, best model saved. (accuracy=0.7573)\n","Train: 100% 2000/2000 [01:52<00:00, 17.70 step/s, accuracy=0.78, loss=0.76, step=32000]\n","Valid: 100% 6944/6944 [00:09<00:00, 745.93 uttr/s, accuracy=0.75, loss=1.03]\n","Train: 100% 2000/2000 [01:52<00:00, 17.81 step/s, accuracy=0.78, loss=0.88, step=34000]\n","Valid: 100% 6944/6944 [00:09<00:00, 728.00 uttr/s, accuracy=0.74, loss=1.06]\n","Train: 100% 2000/2000 [01:53<00:00, 17.64 step/s, accuracy=0.78, loss=0.68, step=36000]\n","Valid: 100% 6944/6944 [00:09<00:00, 726.93 uttr/s, accuracy=0.74, loss=1.06]\n","Train: 100% 2000/2000 [01:54<00:00, 17.49 step/s, accuracy=0.75, loss=1.11, step=38000]\n","Valid: 100% 6944/6944 [00:09<00:00, 725.80 uttr/s, accuracy=0.75, loss=1.04]\n","Train: 100% 2000/2000 [01:54<00:00, 17.41 step/s, accuracy=0.69, loss=1.10, step=4e+4]\n","Valid: 100% 6944/6944 [00:09<00:00, 729.90 uttr/s, accuracy=0.75, loss=1.01]\n","Train:   0% 3/2000 [00:00<02:28, 13.46 step/s, accuracy=0.91, loss=0.54, step=4e+4]Step 40000, best model saved. (accuracy=0.7573)\n","Train: 100% 2000/2000 [01:55<00:00, 17.35 step/s, accuracy=0.72, loss=0.96, step=42000]\n","Valid: 100% 6944/6944 [00:09<00:00, 738.34 uttr/s, accuracy=0.75, loss=1.03]\n","Train: 100% 2000/2000 [01:55<00:00, 17.27 step/s, accuracy=0.59, loss=1.43, step=44000]\n","Valid: 100% 6944/6944 [00:09<00:00, 741.96 uttr/s, accuracy=0.74, loss=1.04]\n","Train: 100% 2000/2000 [01:56<00:00, 17.15 step/s, accuracy=0.91, loss=0.51, step=46000]\n","Valid: 100% 6944/6944 [00:09<00:00, 707.76 uttr/s, accuracy=0.75, loss=1.02]\n","Train: 100% 2000/2000 [01:51<00:00, 17.91 step/s, accuracy=0.72, loss=1.25, step=48000]\n","Valid: 100% 6944/6944 [00:03<00:00, 1814.29 uttr/s, accuracy=0.75, loss=1.03]\n","Train: 100% 2000/2000 [01:23<00:00, 23.89 step/s, accuracy=0.78, loss=0.60, step=5e+4]\n","Valid: 100% 6944/6944 [00:03<00:00, 1838.29 uttr/s, accuracy=0.76, loss=1.00]\n","Train:   0% 5/2000 [00:00<01:30, 22.07 step/s, accuracy=0.81, loss=0.95, step=5e+4]Step 50000, best model saved. (accuracy=0.7586)\n","Train: 100% 2000/2000 [01:23<00:00, 24.00 step/s, accuracy=0.66, loss=1.27, step=52000]\n","Valid: 100% 6944/6944 [00:03<00:00, 1811.02 uttr/s, accuracy=0.75, loss=1.01]\n","Train: 100% 2000/2000 [01:23<00:00, 23.93 step/s, accuracy=0.81, loss=0.96, step=54000]\n","Valid: 100% 6944/6944 [00:03<00:00, 1834.26 uttr/s, accuracy=0.75, loss=1.07]\n","Train: 100% 2000/2000 [01:23<00:00, 23.90 step/s, accuracy=0.75, loss=0.63, step=56000]\n","Valid: 100% 6944/6944 [00:03<00:00, 1823.92 uttr/s, accuracy=0.75, loss=1.02]\n","Train: 100% 2000/2000 [01:23<00:00, 23.91 step/s, accuracy=0.91, loss=0.43, step=58000]\n","Valid: 100% 6944/6944 [00:03<00:00, 1819.78 uttr/s, accuracy=0.76, loss=1.02]\n","Train: 100% 2000/2000 [01:23<00:00, 23.96 step/s, accuracy=0.84, loss=0.49, step=6e+4]\n","Valid: 100% 6944/6944 [00:03<00:00, 1837.80 uttr/s, accuracy=0.74, loss=1.05]\n","Train:   0% 4/2000 [00:00<01:50, 17.98 step/s, accuracy=0.91, loss=0.73, step=6e+4]Step 60000, best model saved. (accuracy=0.7586)\n","Train: 100% 2000/2000 [01:23<00:00, 23.92 step/s, accuracy=0.69, loss=1.22, step=62000]\n","Valid: 100% 6944/6944 [00:03<00:00, 1826.08 uttr/s, accuracy=0.75, loss=1.04]\n","Train: 100% 2000/2000 [01:23<00:00, 24.01 step/s, accuracy=0.62, loss=1.44, step=64000]\n","Valid: 100% 6944/6944 [00:04<00:00, 1694.45 uttr/s, accuracy=0.74, loss=1.06]\n","Train: 100% 2000/2000 [01:24<00:00, 23.69 step/s, accuracy=0.78, loss=0.87, step=66000]\n","Valid: 100% 6944/6944 [00:03<00:00, 1838.27 uttr/s, accuracy=0.74, loss=1.04]\n","Train: 100% 2000/2000 [01:35<00:00, 20.89 step/s, accuracy=0.81, loss=0.89, step=68000]\n","Valid: 100% 6944/6944 [00:45<00:00, 153.77 uttr/s, accuracy=0.75, loss=1.02]\n","Train: 100% 2000/2000 [05:11<00:00,  6.41 step/s, accuracy=0.72, loss=1.53, step=7e+4]\n","Valid: 100% 6944/6944 [00:25<00:00, 277.32 uttr/s, accuracy=0.75, loss=1.03]\n","Train:   0% 1/2000 [00:00<05:50,  5.70 step/s, accuracy=0.75, loss=0.94, step=7e+4]Step 70000, best model saved. (accuracy=0.7586)\n","Train:   2% 31/2000 [00:05<05:40,  5.79 step/s, accuracy=0.78, loss=0.74, step=7e+4]"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-7-ec9055178a63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m   \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m<ipython-input-7-ec9055178a63>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(data_dir, save_path, batch_size, n_workers, valid_steps, warmup_steps, total_steps, save_steps)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# Get data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m       \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m       \u001b[0mtrain_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\pytorch_gpu_v3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\pytorch_gpu_v3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 557\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    558\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\pytorch_gpu_v3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\pytorch_gpu_v3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\pytorch_gpu_v3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-1-ddea6e17c4bd>\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mfeat_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspeaker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# Load preprocessed mel-spectrogram.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mmel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# Segmemt mel-spectrogram into \"segment_len\" frames.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\pytorch_gpu_v3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    580\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\pytorch_gpu_v3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\.conda\\envs\\pytorch_gpu_v3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"0R2rx3AyHpQ-"},"source":["# Inference"]},{"cell_type":"markdown","metadata":{"id":"pSuI3WY9Fz78"},"source":["## Dataset of inference"]},{"cell_type":"code","metadata":{"id":"4evns0055Dsx","executionInfo":{"status":"ok","timestamp":1618239751974,"user_tz":-480,"elapsed":22,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}}},"source":["import os\n","import json\n","import torch\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","\n","\n","class InferenceDataset(Dataset):\n","  def __init__(self, data_dir):\n","    testdata_path = Path(data_dir) / \"testdata.json\"\n","    metadata = json.load(testdata_path.open())\n","    self.data_dir = data_dir\n","    self.data = metadata[\"utterances\"]\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, index):\n","    utterance = self.data[index]\n","    feat_path = utterance[\"feature_path\"]\n","    mel = torch.load(os.path.join(self.data_dir, feat_path))\n","\n","    return feat_path, mel\n","\n","\n","def inference_collate_batch(batch):\n","  \"\"\"Collate a batch of data.\"\"\"\n","  feat_paths, mels = zip(*batch)\n","\n","  return feat_paths, torch.stack(mels)\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oAinHBG1GIWv"},"source":["## Main funcrion of Inference"]},{"cell_type":"code","metadata":{"id":"yQaTt7VDHoRI","colab":{"base_uri":"https://localhost:8080/","height":173,"referenced_widgets":["5d8b2cf8cd9b45e895db75a3c03ea50f","ac24cd21a98c48a6833a3e20408880f1","a84525fe14394f8fbe0b5b431fbf8f40","c609cb1c10984d7fa41bf2dfca5f7491","2c1206b3f0cc4b56aa1d655b91f163f0","f3464a54025242df8c2b2eaf6a4f0f23","af5c3c54648642d9bcb7f55c53a2a473","388e2b89e65f4e2badc528a848b7ce3c"]},"executionInfo":{"status":"ok","timestamp":1618239773935,"user_tz":-480,"elapsed":21977,"user":{"displayName":"楊禮蔚","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOrEH6UNM3_OODfsrE-ZMStDK8nE1jE4kFtrDXeA=s64","userId":"15120520431135783000"}},"outputId":"7687b0ae-c48d-4851-ac85-3cf2b021f3e7","tags":[]},"source":["import json\n","import csv\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","\n","def parse_args():\n","  \"\"\"arguments\"\"\"\n","  config = {\n","    \"data_dir\": \"C:/Users/david/ML/HW4/Dataset\",\n","    \"model_path\": \"C:/Users/david/ML/HW4/modelmodi.ckpt\",\n","    \"output_path\": \"C:/Users/david/ML/HW4/outputmodi.csv\",\n","  }\n","\n","  return config\n","\n","\n","def main(\n","  data_dir,\n","  model_path,\n","  output_path,\n","):\n","  \"\"\"Main function.\"\"\"\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  print(f\"[Info]: Use {device} now!\")\n","\n","  mapping_path = Path(data_dir) / \"mapping.json\"\n","  mapping = json.load(mapping_path.open())\n","\n","  dataset = InferenceDataset(data_dir)\n","  dataloader = DataLoader(\n","    dataset,\n","    batch_size=1,\n","    shuffle=False,\n","    drop_last=False,\n","    num_workers=0,\n","    collate_fn=inference_collate_batch,\n","  )\n","  print(f\"[Info]: Finish loading data!\",flush = True)\n","\n","  speaker_num = len(mapping[\"id2speaker\"])\n","  model = Classifier(n_spks=speaker_num).to(device)\n","  model.load_state_dict(torch.load(model_path))\n","  model.eval()\n","  print(f\"[Info]: Finish creating model!\",flush = True)\n","\n","  results = [[\"Id\", \"Category\"]]\n","  for feat_paths, mels in tqdm(dataloader):\n","    with torch.no_grad():\n","      mels = mels.to(device)\n","      outs = model(mels)\n","      preds = outs.argmax(1).cpu().numpy()\n","      for feat_path, pred in zip(feat_paths, preds):\n","        results.append([feat_path, mapping[\"id2speaker\"][str(pred)]])\n","  \n","  with open(output_path, 'w', newline='') as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerows(results)\n","\n","\n","if __name__ == \"__main__\":\n","  main(**parse_args())\n"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[Info]: Use cuda now!\n","[Info]: Finish loading data!\n","[Info]: Finish creating model!\n","\n","  0%|          | 0/6000 [00:00<?, ?it/s]\u001b[A\n","  0%|          | 8/6000 [00:00<01:21, 73.59it/s]\u001b[A\n","  0%|          | 16/6000 [00:00<01:25, 70.18it/s]\u001b[A\n","  0%|          | 24/6000 [00:00<01:23, 71.99it/s]\u001b[A\n","  1%|          | 32/6000 [00:00<01:21, 73.14it/s]\u001b[A\n","  1%|          | 40/6000 [00:00<01:21, 73.30it/s]\u001b[A\n","  1%|          | 48/6000 [00:00<01:22, 72.27it/s]\u001b[A\n","  1%|          | 56/6000 [00:00<01:22, 71.85it/s]\u001b[A\n","  1%|          | 64/6000 [00:00<01:22, 71.77it/s]\u001b[A\n","  1%|          | 72/6000 [00:00<01:21, 72.54it/s]\u001b[A\n","  1%|▏         | 80/6000 [00:01<01:21, 72.25it/s]\u001b[A\n","  1%|▏         | 88/6000 [00:01<01:22, 71.86it/s]\u001b[A\n","  2%|▏         | 96/6000 [00:01<01:22, 71.79it/s]\u001b[A\n","  2%|▏         | 104/6000 [00:01<01:21, 72.12it/s]\u001b[A\n","  2%|▏         | 112/6000 [00:01<01:26, 68.17it/s]\u001b[A\n","  2%|▏         | 120/6000 [00:01<01:24, 69.90it/s]\u001b[A\n","  2%|▏         | 128/6000 [00:01<01:22, 70.97it/s]\u001b[A\n","  2%|▏         | 136/6000 [00:01<01:24, 69.14it/s]\u001b[A\n","  2%|▏         | 143/6000 [00:02<01:25, 68.31it/s]\u001b[A\n","  2%|▎         | 150/6000 [00:02<01:26, 67.89it/s]\u001b[A\n","  3%|▎         | 157/6000 [00:02<01:25, 68.15it/s]\u001b[A\n","  3%|▎         | 164/6000 [00:02<01:25, 68.15it/s]\u001b[A\n","  3%|▎         | 171/6000 [00:02<01:26, 67.57it/s]\u001b[A\n","  3%|▎         | 179/6000 [00:02<01:24, 69.02it/s]\u001b[A\n","  3%|▎         | 187/6000 [00:02<01:23, 69.82it/s]\u001b[A\n","  3%|▎         | 194/6000 [00:02<01:23, 69.34it/s]\u001b[A\n","  3%|▎         | 202/6000 [00:02<01:23, 69.85it/s]\u001b[A\n","  4%|▎         | 210/6000 [00:02<01:21, 71.17it/s]\u001b[A\n","  4%|▎         | 218/6000 [00:03<01:20, 72.10it/s]\u001b[A\n","  4%|▍         | 226/6000 [00:03<01:18, 73.56it/s]\u001b[A\n","  4%|▍         | 234/6000 [00:03<01:18, 73.37it/s]\u001b[A\n","  4%|▍         | 242/6000 [00:03<01:17, 74.26it/s]\u001b[A\n","  4%|▍         | 250/6000 [00:03<01:16, 74.89it/s]\u001b[A\n","  4%|▍         | 258/6000 [00:03<01:17, 74.49it/s]\u001b[A\n","  4%|▍         | 266/6000 [00:03<01:16, 75.06it/s]\u001b[A\n","  5%|▍         | 274/6000 [00:03<01:15, 75.45it/s]\u001b[A\n","  5%|▍         | 282/6000 [00:03<01:15, 75.52it/s]\u001b[A\n","  5%|▍         | 290/6000 [00:04<01:15, 75.78it/s]\u001b[A\n","  5%|▍         | 298/6000 [00:04<01:15, 75.32it/s]\u001b[A\n","  5%|▌         | 306/6000 [00:04<01:15, 75.43it/s]\u001b[A\n","  5%|▌         | 314/6000 [00:04<01:15, 75.29it/s]\u001b[A\n","  5%|▌         | 322/6000 [00:04<01:15, 75.40it/s]\u001b[A\n","  6%|▌         | 330/6000 [00:04<01:15, 75.48it/s]\u001b[A\n","  6%|▌         | 338/6000 [00:04<01:15, 75.33it/s]\u001b[A\n","  6%|▌         | 346/6000 [00:04<01:15, 75.22it/s]\u001b[A\n","  6%|▌         | 354/6000 [00:04<01:14, 75.35it/s]\u001b[A\n","  6%|▌         | 362/6000 [00:04<01:14, 75.45it/s]\u001b[A\n","  6%|▌         | 370/6000 [00:05<01:15, 74.26it/s]\u001b[A\n","  6%|▋         | 378/6000 [00:05<01:15, 74.06it/s]\u001b[A\n","  6%|▋         | 386/6000 [00:05<01:15, 74.53it/s]\u001b[A\n","  7%|▋         | 394/6000 [00:05<01:16, 73.23it/s]\u001b[A\n","  7%|▋         | 402/6000 [00:05<01:16, 72.94it/s]\u001b[A\n","  7%|▋         | 410/6000 [00:05<01:18, 71.38it/s]\u001b[A\n","  7%|▋         | 418/6000 [00:05<01:18, 70.88it/s]\u001b[A\n","  7%|▋         | 426/6000 [00:05<01:19, 69.80it/s]\u001b[A\n","  7%|▋         | 433/6000 [00:05<01:19, 69.72it/s]\u001b[A\n","  7%|▋         | 441/6000 [00:06<01:18, 71.06it/s]\u001b[A\n","  7%|▋         | 449/6000 [00:06<01:17, 71.81it/s]\u001b[A\n","  8%|▊         | 457/6000 [00:06<01:16, 72.74it/s]\u001b[A\n","  8%|▊         | 465/6000 [00:06<01:16, 72.40it/s]\u001b[A\n","  8%|▊         | 473/6000 [00:06<01:15, 73.36it/s]\u001b[A\n","  8%|▊         | 481/6000 [00:06<01:16, 72.43it/s]\u001b[A\n","  8%|▊         | 489/6000 [00:06<01:15, 73.17it/s]\u001b[A\n","  8%|▊         | 497/6000 [00:06<01:15, 73.09it/s]\u001b[A\n","  8%|▊         | 505/6000 [00:06<01:15, 73.05it/s]\u001b[A\n","  9%|▊         | 513/6000 [00:07<01:15, 72.61it/s]\u001b[A\n","  9%|▊         | 521/6000 [00:07<01:14, 73.86it/s]\u001b[A\n","  9%|▉         | 529/6000 [00:07<01:14, 73.89it/s]\u001b[A\n","  9%|▉         | 537/6000 [00:07<01:13, 74.01it/s]\u001b[A\n","  9%|▉         | 545/6000 [00:07<01:13, 74.09it/s]\u001b[A\n","  9%|▉         | 553/6000 [00:07<01:13, 73.73it/s]\u001b[A\n","  9%|▉         | 561/6000 [00:07<01:13, 73.69it/s]\u001b[A\n","  9%|▉         | 569/6000 [00:07<01:14, 73.26it/s]\u001b[A\n"," 10%|▉         | 577/6000 [00:07<01:13, 73.56it/s]\u001b[A\n"," 10%|▉         | 585/6000 [00:08<01:14, 72.97it/s]\u001b[A\n"," 10%|▉         | 593/6000 [00:08<01:13, 73.76it/s]\u001b[A\n"," 10%|█         | 601/6000 [00:08<01:13, 73.50it/s]\u001b[A\n"," 10%|█         | 609/6000 [00:08<01:13, 73.73it/s]\u001b[A\n"," 10%|█         | 617/6000 [00:08<01:13, 73.09it/s]\u001b[A\n"," 10%|█         | 625/6000 [00:08<01:13, 73.44it/s]\u001b[A\n"," 11%|█         | 633/6000 [00:08<01:13, 72.68it/s]\u001b[A\n"," 11%|█         | 641/6000 [00:08<01:12, 73.96it/s]\u001b[A\n"," 11%|█         | 649/6000 [00:08<01:12, 73.65it/s]\u001b[A\n"," 11%|█         | 657/6000 [00:09<01:12, 74.03it/s]\u001b[A\n"," 11%|█         | 665/6000 [00:09<01:12, 73.50it/s]\u001b[A\n"," 11%|█         | 673/6000 [00:09<01:13, 72.73it/s]\u001b[A\n"," 11%|█▏        | 681/6000 [00:09<01:13, 72.00it/s]\u001b[A\n"," 11%|█▏        | 689/6000 [00:09<01:12, 73.47it/s]\u001b[A\n"," 12%|█▏        | 697/6000 [00:09<01:11, 73.71it/s]\u001b[A\n"," 12%|█▏        | 705/6000 [00:09<01:11, 73.67it/s]\u001b[A\n"," 12%|█▏        | 713/6000 [00:09<01:11, 73.65it/s]\u001b[A\n"," 12%|█▏        | 721/6000 [00:09<01:12, 72.63it/s]\u001b[A\n"," 12%|█▏        | 729/6000 [00:10<01:16, 68.61it/s]\u001b[A\n"," 12%|█▏        | 737/6000 [00:10<01:15, 69.85it/s]\u001b[A\n"," 12%|█▏        | 745/6000 [00:10<01:14, 70.93it/s]\u001b[A\n"," 13%|█▎        | 753/6000 [00:10<01:12, 71.90it/s]\u001b[A\n"," 13%|█▎        | 761/6000 [00:10<01:11, 72.79it/s]\u001b[A\n"," 13%|█▎        | 769/6000 [00:10<01:11, 73.03it/s]\u001b[A\n"," 13%|█▎        | 777/6000 [00:10<01:12, 72.02it/s]\u001b[A\n"," 13%|█▎        | 785/6000 [00:10<01:11, 72.48it/s]\u001b[A\n"," 13%|█▎        | 793/6000 [00:10<01:10, 73.41it/s]\u001b[A\n"," 13%|█▎        | 801/6000 [00:11<01:10, 73.26it/s]\u001b[A\n"," 13%|█▎        | 809/6000 [00:11<01:10, 73.56it/s]\u001b[A\n"," 14%|█▎        | 817/6000 [00:11<01:11, 72.57it/s]\u001b[A\n"," 14%|█▍        | 825/6000 [00:11<01:10, 73.07it/s]\u001b[A\n"," 14%|█▍        | 833/6000 [00:11<01:11, 72.04it/s]\u001b[A\n"," 14%|█▍        | 841/6000 [00:11<01:10, 73.10it/s]\u001b[A\n"," 14%|█▍        | 849/6000 [00:11<01:10, 73.04it/s]\u001b[A\n"," 14%|█▍        | 857/6000 [00:11<01:10, 73.01it/s]\u001b[A\n"," 14%|█▍        | 865/6000 [00:11<01:10, 72.78it/s]\u001b[A\n"," 15%|█▍        | 873/6000 [00:12<01:10, 72.82it/s]\u001b[A\n"," 15%|█▍        | 881/6000 [00:12<01:10, 72.46it/s]\u001b[A\n"," 15%|█▍        | 889/6000 [00:12<01:10, 72.60it/s]\u001b[A\n"," 15%|█▍        | 897/6000 [00:12<01:09, 73.09it/s]\u001b[A\n"," 15%|█▌        | 905/6000 [00:12<01:09, 73.64it/s]\u001b[A\n"," 15%|█▌        | 913/6000 [00:12<01:09, 73.43it/s]\u001b[A\n"," 15%|█▌        | 921/6000 [00:12<01:09, 73.07it/s]\u001b[A\n"," 15%|█▌        | 929/6000 [00:12<01:09, 73.03it/s]\u001b[A\n"," 16%|█▌        | 937/6000 [00:12<01:09, 73.20it/s]\u001b[A\n"," 16%|█▌        | 945/6000 [00:13<01:09, 72.72it/s]\u001b[A\n"," 16%|█▌        | 953/6000 [00:13<01:08, 73.38it/s]\u001b[A\n"," 16%|█▌        | 961/6000 [00:13<01:08, 73.64it/s]\u001b[A\n"," 16%|█▌        | 969/6000 [00:13<01:09, 72.82it/s]\u001b[A\n"," 16%|█▋        | 977/6000 [00:13<01:08, 73.25it/s]\u001b[A\n"," 16%|█▋        | 985/6000 [00:13<01:08, 73.55it/s]\u001b[A\n"," 17%|█▋        | 993/6000 [00:13<01:07, 74.18it/s]\u001b[A\n"," 17%|█▋        | 1001/6000 [00:13<01:07, 73.60it/s]\u001b[A\n"," 17%|█▋        | 1009/6000 [00:13<01:07, 73.80it/s]\u001b[A\n"," 17%|█▋        | 1017/6000 [00:13<01:07, 73.33it/s]\u001b[A\n"," 17%|█▋        | 1025/6000 [00:14<01:07, 73.81it/s]\u001b[A\n"," 17%|█▋        | 1033/6000 [00:14<01:07, 73.94it/s]\u001b[A\n"," 17%|█▋        | 1041/6000 [00:14<01:07, 73.24it/s]\u001b[A\n"," 17%|█▋        | 1049/6000 [00:14<01:07, 73.54it/s]\u001b[A\n"," 18%|█▊        | 1057/6000 [00:14<01:06, 74.21it/s]\u001b[A\n"," 18%|█▊        | 1065/6000 [00:14<01:07, 73.62it/s]\u001b[A\n"," 18%|█▊        | 1073/6000 [00:14<01:06, 73.81it/s]\u001b[A\n"," 18%|█▊        | 1081/6000 [00:14<01:06, 73.95it/s]\u001b[A\n"," 18%|█▊        | 1089/6000 [00:14<01:07, 72.64it/s]\u001b[A\n"," 18%|█▊        | 1097/6000 [00:15<01:06, 73.32it/s]\u001b[A\n"," 18%|█▊        | 1105/6000 [00:15<01:05, 74.22it/s]\u001b[A\n"," 19%|█▊        | 1113/6000 [00:15<01:06, 73.22it/s]\u001b[A\n"," 19%|█▊        | 1121/6000 [00:15<01:06, 73.73it/s]\u001b[A\n"," 19%|█▉        | 1129/6000 [00:15<01:06, 73.49it/s]\u001b[A\n"," 19%|█▉        | 1137/6000 [00:15<01:06, 73.12it/s]\u001b[A\n"," 19%|█▉        | 1145/6000 [00:15<01:06, 73.46it/s]\u001b[A\n"," 19%|█▉        | 1153/6000 [00:15<01:05, 73.91it/s]\u001b[A\n"," 19%|█▉        | 1161/6000 [00:15<01:05, 74.40it/s]\u001b[A\n"," 19%|█▉        | 1169/6000 [00:16<01:04, 74.37it/s]\u001b[A\n"," 20%|█▉        | 1177/6000 [00:16<01:04, 74.55it/s]\u001b[A\n"," 20%|█▉        | 1185/6000 [00:16<01:05, 73.24it/s]\u001b[A\n"," 20%|█▉        | 1193/6000 [00:16<01:05, 72.95it/s]\u001b[A\n"," 20%|██        | 1201/6000 [00:16<01:05, 72.74it/s]\u001b[A\n"," 20%|██        | 1209/6000 [00:16<01:05, 72.79it/s]\u001b[A\n"," 20%|██        | 1217/6000 [00:16<01:05, 73.23it/s]\u001b[A\n"," 20%|██        | 1225/6000 [00:16<01:04, 73.74it/s]\u001b[A\n"," 21%|██        | 1233/6000 [00:16<01:04, 73.70it/s]\u001b[A\n"," 21%|██        | 1241/6000 [00:17<01:04, 73.46it/s]\u001b[A\n"," 21%|██        | 1249/6000 [00:17<01:04, 73.50it/s]\u001b[A\n"," 21%|██        | 1257/6000 [00:17<01:04, 73.53it/s]\u001b[A\n"," 21%|██        | 1265/6000 [00:17<01:04, 73.34it/s]\u001b[A\n"," 21%|██        | 1273/6000 [00:17<01:04, 73.02it/s]\u001b[A\n"," 21%|██▏       | 1281/6000 [00:17<01:04, 72.79it/s]\u001b[A\n"," 21%|██▏       | 1289/6000 [00:17<01:04, 72.83it/s]\u001b[A\n"," 22%|██▏       | 1297/6000 [00:17<01:04, 73.06it/s]\u001b[A\n"," 22%|██▏       | 1305/6000 [00:17<01:05, 72.23it/s]\u001b[A\n"," 22%|██▏       | 1313/6000 [00:18<01:04, 72.63it/s]\u001b[A\n"," 22%|██▏       | 1321/6000 [00:18<01:04, 72.92it/s]\u001b[A\n"," 22%|██▏       | 1329/6000 [00:18<01:04, 72.72it/s]\u001b[A\n"," 22%|██▏       | 1337/6000 [00:18<01:04, 72.58it/s]\u001b[A\n"," 22%|██▏       | 1345/6000 [00:18<01:04, 72.10it/s]\u001b[A\n"," 23%|██▎       | 1353/6000 [00:18<01:03, 73.54it/s]\u001b[A\n"," 23%|██▎       | 1361/6000 [00:18<01:03, 72.95it/s]\u001b[A\n"," 23%|██▎       | 1369/6000 [00:18<01:02, 73.54it/s]\u001b[A\n"," 23%|██▎       | 1377/6000 [00:18<01:03, 73.36it/s]\u001b[A\n"," 23%|██▎       | 1385/6000 [00:19<01:02, 73.42it/s]\u001b[A\n"," 23%|██▎       | 1393/6000 [00:19<01:02, 73.88it/s]\u001b[A\n"," 23%|██▎       | 1401/6000 [00:19<01:02, 73.59it/s]\u001b[A\n"," 23%|██▎       | 1409/6000 [00:19<01:02, 73.59it/s]\u001b[A\n"," 24%|██▎       | 1417/6000 [00:19<01:01, 74.00it/s]\u001b[A\n"," 24%|██▍       | 1428/6000 [00:19<00:54, 84.04it/s]\u001b[A\n"," 24%|██▍       | 1440/6000 [00:19<00:48, 94.21it/s]\u001b[A\n"," 24%|██▍       | 1453/6000 [00:19<00:43, 103.50it/s]\u001b[A\n"," 24%|██▍       | 1465/6000 [00:19<00:42, 107.77it/s]\u001b[A\n"," 25%|██▍       | 1478/6000 [00:19<00:39, 113.38it/s]\u001b[A\n"," 25%|██▍       | 1491/6000 [00:20<00:38, 115.97it/s]\u001b[A\n"," 25%|██▌       | 1503/6000 [00:20<00:38, 115.89it/s]\u001b[A\n"," 25%|██▌       | 1516/6000 [00:20<00:37, 119.06it/s]\u001b[A\n"," 25%|██▌       | 1528/6000 [00:20<00:37, 118.39it/s]\u001b[A\n"," 26%|██▌       | 1540/6000 [00:20<00:37, 118.61it/s]\u001b[A\n"," 26%|██▌       | 1552/6000 [00:20<00:37, 118.07it/s]\u001b[A\n"," 26%|██▌       | 1564/6000 [00:20<00:37, 117.35it/s]\u001b[A\n"," 26%|██▋       | 1576/6000 [00:20<00:37, 117.53it/s]\u001b[A\n"," 26%|██▋       | 1589/6000 [00:20<00:36, 120.59it/s]\u001b[A\n"," 27%|██▋       | 1602/6000 [00:20<00:36, 121.67it/s]\u001b[A\n"," 27%|██▋       | 1615/6000 [00:21<00:36, 120.68it/s]\u001b[A\n"," 27%|██▋       | 1628/6000 [00:21<00:35, 121.71it/s]\u001b[A\n"," 27%|██▋       | 1641/6000 [00:21<00:35, 121.40it/s]\u001b[A\n"," 28%|██▊       | 1654/6000 [00:21<00:36, 120.51it/s]\u001b[A\n"," 28%|██▊       | 1667/6000 [00:21<00:36, 119.23it/s]\u001b[A\n"," 28%|██▊       | 1679/6000 [00:21<00:36, 119.20it/s]\u001b[A\n"," 28%|██▊       | 1692/6000 [00:21<00:35, 119.99it/s]\u001b[A\n"," 28%|██▊       | 1705/6000 [00:21<00:36, 118.22it/s]\u001b[A\n"," 29%|██▊       | 1717/6000 [00:21<00:37, 114.29it/s]\u001b[A\n"," 29%|██▉       | 1729/6000 [00:22<00:36, 115.65it/s]\u001b[A\n"," 29%|██▉       | 1742/6000 [00:22<00:36, 117.50it/s]\u001b[A\n"," 29%|██▉       | 1754/6000 [00:22<00:35, 117.96it/s]\u001b[A\n"," 29%|██▉       | 1766/6000 [00:22<00:36, 115.97it/s]\u001b[A\n"," 30%|██▉       | 1778/6000 [00:22<00:36, 114.26it/s]\u001b[A\n"," 30%|██▉       | 1790/6000 [00:22<00:36, 115.01it/s]\u001b[A\n"," 30%|███       | 1802/6000 [00:22<00:36, 114.56it/s]\u001b[A\n"," 30%|███       | 1814/6000 [00:22<00:36, 115.22it/s]\u001b[A\n"," 30%|███       | 1826/6000 [00:22<00:37, 112.46it/s]\u001b[A\n"," 31%|███       | 1839/6000 [00:23<00:36, 114.94it/s]\u001b[A\n"," 31%|███       | 1851/6000 [00:23<00:35, 116.14it/s]\u001b[A\n"," 31%|███       | 1864/6000 [00:23<00:34, 118.20it/s]\u001b[A\n"," 31%|███▏      | 1877/6000 [00:23<00:34, 119.97it/s]\u001b[A\n"," 32%|███▏      | 1890/6000 [00:23<00:34, 117.54it/s]\u001b[A\n"," 32%|███▏      | 1902/6000 [00:23<00:35, 115.07it/s]\u001b[A\n"," 32%|███▏      | 1915/6000 [00:23<00:34, 116.74it/s]\u001b[A\n"," 32%|███▏      | 1927/6000 [00:23<00:34, 116.76it/s]\u001b[A\n"," 32%|███▏      | 1939/6000 [00:23<00:34, 116.12it/s]\u001b[A\n"," 33%|███▎      | 1951/6000 [00:23<00:34, 116.32it/s]\u001b[A\n"," 33%|███▎      | 1963/6000 [00:24<00:34, 116.14it/s]\u001b[A\n"," 33%|███▎      | 1975/6000 [00:24<00:34, 116.67it/s]\u001b[A\n"," 33%|███▎      | 1987/6000 [00:24<00:34, 116.38it/s]\u001b[A\n"," 33%|███▎      | 1999/6000 [00:24<00:34, 115.18it/s]\u001b[A\n"," 34%|███▎      | 2012/6000 [00:24<00:33, 118.55it/s]\u001b[A\n"," 34%|███▎      | 2024/6000 [00:24<00:34, 116.02it/s]\u001b[A\n"," 34%|███▍      | 2036/6000 [00:24<00:34, 113.98it/s]\u001b[A\n"," 34%|███▍      | 2048/6000 [00:24<00:34, 113.52it/s]\u001b[A\n"," 34%|███▍      | 2060/6000 [00:24<00:34, 113.20it/s]\u001b[A\n"," 35%|███▍      | 2072/6000 [00:25<00:34, 113.93it/s]\u001b[A\n"," 35%|███▍      | 2084/6000 [00:25<00:34, 112.85it/s]\u001b[A\n"," 35%|███▍      | 2096/6000 [00:25<00:34, 114.33it/s]\u001b[A\n"," 35%|███▌      | 2109/6000 [00:25<00:33, 116.60it/s]\u001b[A\n"," 35%|███▌      | 2122/6000 [00:25<00:32, 117.85it/s]\u001b[A\n"," 36%|███▌      | 2135/6000 [00:25<00:32, 118.38it/s]\u001b[A\n"," 36%|███▌      | 2148/6000 [00:25<00:32, 119.07it/s]\u001b[A\n"," 36%|███▌      | 2160/6000 [00:25<00:32, 119.09it/s]\u001b[A\n"," 36%|███▌      | 2172/6000 [00:25<00:32, 119.10it/s]\u001b[A\n"," 36%|███▋      | 2185/6000 [00:25<00:31, 120.28it/s]\u001b[A\n"," 37%|███▋      | 2198/6000 [00:26<00:31, 121.79it/s]\u001b[A\n"," 37%|███▋      | 2211/6000 [00:26<00:31, 122.14it/s]\u001b[A\n"," 37%|███▋      | 2224/6000 [00:26<00:30, 123.10it/s]\u001b[A\n"," 37%|███▋      | 2237/6000 [00:26<00:31, 121.32it/s]\u001b[A\n"," 38%|███▊      | 2250/6000 [00:26<00:31, 119.13it/s]\u001b[A\n"," 38%|███▊      | 2262/6000 [00:26<00:31, 117.47it/s]\u001b[A\n"," 38%|███▊      | 2275/6000 [00:26<00:31, 120.11it/s]\u001b[A\n"," 38%|███▊      | 2288/6000 [00:26<00:30, 120.29it/s]\u001b[A\n"," 38%|███▊      | 2301/6000 [00:26<00:31, 118.42it/s]\u001b[A\n"," 39%|███▊      | 2313/6000 [00:27<00:31, 118.62it/s]\u001b[A\n"," 39%|███▉      | 2325/6000 [00:27<00:31, 118.43it/s]\u001b[A\n"," 39%|███▉      | 2337/6000 [00:27<00:31, 117.95it/s]\u001b[A\n"," 39%|███▉      | 2349/6000 [00:27<00:31, 116.94it/s]\u001b[A\n"," 39%|███▉      | 2362/6000 [00:27<00:30, 118.43it/s]\u001b[A\n"," 40%|███▉      | 2374/6000 [00:27<00:31, 116.95it/s]\u001b[A\n"," 40%|███▉      | 2387/6000 [00:27<00:30, 118.76it/s]\u001b[A\n"," 40%|████      | 2400/6000 [00:27<00:29, 120.37it/s]\u001b[A\n"," 40%|████      | 2413/6000 [00:27<00:29, 120.81it/s]\u001b[A\n"," 40%|████      | 2426/6000 [00:27<00:29, 121.11it/s]\u001b[A\n"," 41%|████      | 2439/6000 [00:28<00:29, 119.97it/s]\u001b[A\n"," 41%|████      | 2452/6000 [00:28<00:29, 121.20it/s]\u001b[A\n"," 41%|████      | 2465/6000 [00:28<00:28, 122.07it/s]\u001b[A\n"," 41%|████▏     | 2478/6000 [00:28<00:28, 121.65it/s]\u001b[A\n"," 42%|████▏     | 2491/6000 [00:28<00:28, 121.70it/s]\u001b[A\n"," 42%|████▏     | 2504/6000 [00:28<00:29, 120.04it/s]\u001b[A\n"," 42%|████▏     | 2517/6000 [00:28<00:28, 120.58it/s]\u001b[A\n"," 42%|████▏     | 2530/6000 [00:28<00:28, 120.62it/s]\u001b[A\n"," 42%|████▏     | 2543/6000 [00:28<00:28, 121.99it/s]\u001b[A\n"," 43%|████▎     | 2556/6000 [00:29<00:28, 120.92it/s]\u001b[A\n"," 43%|████▎     | 2569/6000 [00:29<00:28, 121.87it/s]\u001b[A\n"," 43%|████▎     | 2582/6000 [00:29<00:28, 121.51it/s]\u001b[A\n"," 43%|████▎     | 2595/6000 [00:29<00:28, 121.27it/s]\u001b[A\n"," 43%|████▎     | 2608/6000 [00:29<00:28, 120.76it/s]\u001b[A\n"," 44%|████▎     | 2621/6000 [00:29<00:27, 121.07it/s]\u001b[A\n"," 44%|████▍     | 2634/6000 [00:29<00:27, 120.29it/s]\u001b[A\n"," 44%|████▍     | 2647/6000 [00:29<00:28, 119.75it/s]\u001b[A\n"," 44%|████▍     | 2659/6000 [00:29<00:28, 119.23it/s]\u001b[A\n"," 45%|████▍     | 2672/6000 [00:30<00:27, 119.68it/s]\u001b[A\n"," 45%|████▍     | 2684/6000 [00:30<00:28, 118.17it/s]\u001b[A\n"," 45%|████▍     | 2697/6000 [00:30<00:27, 119.27it/s]\u001b[A\n"," 45%|████▌     | 2710/6000 [00:30<00:27, 119.70it/s]\u001b[A\n"," 45%|████▌     | 2723/6000 [00:30<00:26, 121.70it/s]\u001b[A\n"," 46%|████▌     | 2736/6000 [00:30<00:27, 119.71it/s]\u001b[A\n"," 46%|████▌     | 2748/6000 [00:30<00:27, 118.87it/s]\u001b[A\n"," 46%|████▌     | 2760/6000 [00:30<00:27, 118.94it/s]\u001b[A\n"," 46%|████▌     | 2772/6000 [00:30<00:27, 118.65it/s]\u001b[A\n"," 46%|████▋     | 2784/6000 [00:30<00:27, 117.77it/s]\u001b[A\n"," 47%|████▋     | 2797/6000 [00:31<00:26, 119.01it/s]\u001b[A\n"," 47%|████▋     | 2809/6000 [00:31<00:26, 118.36it/s]\u001b[A\n"," 47%|████▋     | 2821/6000 [00:31<00:27, 116.89it/s]\u001b[A\n"," 47%|████▋     | 2833/6000 [00:31<00:27, 116.87it/s]\u001b[A\n"," 47%|████▋     | 2845/6000 [00:31<00:27, 115.19it/s]\u001b[A\n"," 48%|████▊     | 2857/6000 [00:31<00:27, 114.03it/s]\u001b[A\n"," 48%|████▊     | 2869/6000 [00:31<00:27, 114.52it/s]\u001b[A\n"," 48%|████▊     | 2881/6000 [00:31<00:27, 115.20it/s]\u001b[A\n"," 48%|████▊     | 2893/6000 [00:31<00:27, 114.36it/s]\u001b[A\n"," 48%|████▊     | 2906/6000 [00:32<00:26, 116.29it/s]\u001b[A\n"," 49%|████▊     | 2919/6000 [00:32<00:26, 116.97it/s]\u001b[A\n"," 49%|████▉     | 2931/6000 [00:32<00:26, 115.62it/s]\u001b[A\n"," 49%|████▉     | 2944/6000 [00:32<00:26, 117.47it/s]\u001b[A\n"," 49%|████▉     | 2956/6000 [00:32<00:26, 114.68it/s]\u001b[A\n"," 49%|████▉     | 2968/6000 [00:32<00:26, 113.70it/s]\u001b[A\n"," 50%|████▉     | 2981/6000 [00:32<00:26, 115.80it/s]\u001b[A\n"," 50%|████▉     | 2993/6000 [00:32<00:29, 102.79it/s]\u001b[A\n"," 50%|█████     | 3005/6000 [00:32<00:28, 106.27it/s]\u001b[A\n"," 50%|█████     | 3018/6000 [00:33<00:26, 111.58it/s]\u001b[A\n"," 50%|█████     | 3030/6000 [00:33<00:26, 111.53it/s]\u001b[A\n"," 51%|█████     | 3042/6000 [00:33<00:26, 112.11it/s]\u001b[A\n"," 51%|█████     | 3054/6000 [00:33<00:26, 112.83it/s]\u001b[A\n"," 51%|█████     | 3066/6000 [00:33<00:25, 113.67it/s]\u001b[A\n"," 51%|█████▏    | 3078/6000 [00:33<00:25, 113.30it/s]\u001b[A\n"," 52%|█████▏    | 3090/6000 [00:33<00:25, 114.00it/s]\u001b[A\n"," 52%|█████▏    | 3102/6000 [00:33<00:25, 113.54it/s]\u001b[A\n"," 52%|█████▏    | 3114/6000 [00:33<00:25, 114.50it/s]\u001b[A\n"," 52%|█████▏    | 3126/6000 [00:33<00:24, 115.51it/s]\u001b[A\n"," 52%|█████▏    | 3138/6000 [00:34<00:25, 113.93it/s]\u001b[A\n"," 52%|█████▎    | 3150/6000 [00:34<00:24, 114.78it/s]\u001b[A\n"," 53%|█████▎    | 3162/6000 [00:34<00:25, 113.43it/s]\u001b[A\n"," 53%|█████▎    | 3174/6000 [00:34<00:24, 114.10it/s]\u001b[A\n"," 53%|█████▎    | 3186/6000 [00:34<00:24, 115.23it/s]\u001b[A\n"," 53%|█████▎    | 3198/6000 [00:34<00:24, 115.70it/s]\u001b[A\n"," 54%|█████▎    | 3210/6000 [00:34<00:28, 96.26it/s] \u001b[A\n"," 54%|█████▎    | 3222/6000 [00:34<00:27, 101.88it/s]\u001b[A\n"," 54%|█████▍    | 3234/6000 [00:34<00:26, 106.23it/s]\u001b[A\n"," 54%|█████▍    | 3246/6000 [00:35<00:25, 109.20it/s]\u001b[A\n"," 54%|█████▍    | 3258/6000 [00:35<00:24, 112.00it/s]\u001b[A\n"," 55%|█████▍    | 3271/6000 [00:35<00:23, 115.91it/s]\u001b[A\n"," 55%|█████▍    | 3283/6000 [00:35<00:23, 116.84it/s]\u001b[A\n"," 55%|█████▍    | 3295/6000 [00:35<00:23, 115.50it/s]\u001b[A\n"," 55%|█████▌    | 3307/6000 [00:35<00:23, 113.29it/s]\u001b[A\n"," 55%|█████▌    | 3319/6000 [00:35<00:23, 113.68it/s]\u001b[A\n"," 56%|█████▌    | 3331/6000 [00:35<00:23, 111.74it/s]\u001b[A\n"," 56%|█████▌    | 3343/6000 [00:35<00:23, 113.21it/s]\u001b[A\n"," 56%|█████▌    | 3355/6000 [00:36<00:23, 113.30it/s]\u001b[A\n"," 56%|█████▌    | 3367/6000 [00:36<00:23, 114.01it/s]\u001b[A\n"," 56%|█████▋    | 3379/6000 [00:36<00:23, 113.86it/s]\u001b[A\n"," 57%|█████▋    | 3391/6000 [00:36<00:23, 111.85it/s]\u001b[A\n"," 57%|█████▋    | 3403/6000 [00:36<00:22, 113.62it/s]\u001b[A\n"," 57%|█████▋    | 3415/6000 [00:36<00:22, 115.21it/s]\u001b[A\n"," 57%|█████▋    | 3427/6000 [00:36<00:22, 116.36it/s]\u001b[A\n"," 57%|█████▋    | 3440/6000 [00:36<00:21, 117.36it/s]\u001b[A\n"," 58%|█████▊    | 3452/6000 [00:36<00:21, 117.20it/s]\u001b[A\n"," 58%|█████▊    | 3464/6000 [00:36<00:21, 116.42it/s]\u001b[A\n"," 58%|█████▊    | 3476/6000 [00:37<00:21, 116.53it/s]\u001b[A\n"," 58%|█████▊    | 3488/6000 [00:37<00:21, 115.29it/s]\u001b[A\n"," 58%|█████▊    | 3500/6000 [00:37<00:21, 116.07it/s]\u001b[A\n"," 59%|█████▊    | 3512/6000 [00:37<00:21, 115.96it/s]\u001b[A\n"," 59%|█████▊    | 3524/6000 [00:37<00:21, 113.91it/s]\u001b[A\n"," 59%|█████▉    | 3536/6000 [00:37<00:22, 111.27it/s]\u001b[A\n"," 59%|█████▉    | 3548/6000 [00:37<00:22, 110.39it/s]\u001b[A\n"," 59%|█████▉    | 3560/6000 [00:37<00:22, 109.49it/s]\u001b[A\n"," 60%|█████▉    | 3572/6000 [00:37<00:22, 110.36it/s]\u001b[A\n"," 60%|█████▉    | 3584/6000 [00:38<00:21, 111.91it/s]\u001b[A\n"," 60%|█████▉    | 3597/6000 [00:38<00:20, 115.51it/s]\u001b[A\n"," 60%|██████    | 3609/6000 [00:38<00:20, 115.24it/s]\u001b[A\n"," 60%|██████    | 3622/6000 [00:38<00:20, 117.22it/s]\u001b[A\n"," 61%|██████    | 3635/6000 [00:38<00:19, 118.27it/s]\u001b[A\n"," 61%|██████    | 3647/6000 [00:38<00:20, 116.53it/s]\u001b[A\n"," 61%|██████    | 3660/6000 [00:38<00:19, 117.46it/s]\u001b[A\n"," 61%|██████    | 3672/6000 [00:38<00:20, 113.74it/s]\u001b[A\n"," 61%|██████▏   | 3684/6000 [00:38<00:21, 106.32it/s]\u001b[A\n"," 62%|██████▏   | 3697/6000 [00:39<00:20, 110.13it/s]\u001b[A\n"," 62%|██████▏   | 3709/6000 [00:39<00:20, 109.62it/s]\u001b[A\n"," 62%|██████▏   | 3722/6000 [00:39<00:20, 113.14it/s]\u001b[A\n"," 62%|██████▏   | 3735/6000 [00:39<00:19, 115.99it/s]\u001b[A\n"," 62%|██████▏   | 3747/6000 [00:39<00:19, 116.88it/s]\u001b[A\n"," 63%|██████▎   | 3760/6000 [00:39<00:18, 119.70it/s]\u001b[A\n"," 63%|██████▎   | 3773/6000 [00:39<00:18, 119.67it/s]\u001b[A\n"," 63%|██████▎   | 3785/6000 [00:39<00:18, 119.17it/s]\u001b[A\n"," 63%|██████▎   | 3797/6000 [00:39<00:18, 119.16it/s]\u001b[A\n"," 63%|██████▎   | 3809/6000 [00:39<00:18, 119.15it/s]\u001b[A\n"," 64%|██████▎   | 3821/6000 [00:40<00:18, 118.80it/s]\u001b[A\n"," 64%|██████▍   | 3833/6000 [00:40<00:18, 117.86it/s]\u001b[A\n"," 64%|██████▍   | 3846/6000 [00:40<00:18, 118.74it/s]\u001b[A\n"," 64%|██████▍   | 3858/6000 [00:40<00:18, 118.51it/s]\u001b[A\n"," 65%|██████▍   | 3871/6000 [00:40<00:17, 119.52it/s]\u001b[A\n"," 65%|██████▍   | 3884/6000 [00:40<00:17, 120.56it/s]\u001b[A\n"," 65%|██████▍   | 3897/6000 [00:40<00:17, 123.04it/s]\u001b[A\n"," 65%|██████▌   | 3910/6000 [00:40<00:17, 122.67it/s]\u001b[A\n"," 65%|██████▌   | 3923/6000 [00:40<00:17, 121.37it/s]\u001b[A\n"," 66%|██████▌   | 3936/6000 [00:41<00:16, 122.20it/s]\u001b[A\n"," 66%|██████▌   | 3949/6000 [00:41<00:16, 122.08it/s]\u001b[A\n"," 66%|██████▌   | 3962/6000 [00:41<00:16, 122.00it/s]\u001b[A\n"," 66%|██████▋   | 3975/6000 [00:41<00:16, 120.59it/s]\u001b[A\n"," 66%|██████▋   | 3988/6000 [00:41<00:16, 119.62it/s]\u001b[A\n"," 67%|██████▋   | 4001/6000 [00:41<00:16, 121.62it/s]\u001b[A\n"," 67%|██████▋   | 4014/6000 [00:41<00:16, 122.71it/s]\u001b[A\n"," 67%|██████▋   | 4027/6000 [00:41<00:16, 122.79it/s]\u001b[A\n"," 67%|██████▋   | 4040/6000 [00:41<00:15, 122.84it/s]\u001b[A\n"," 68%|██████▊   | 4053/6000 [00:41<00:16, 120.50it/s]\u001b[A\n"," 68%|██████▊   | 4066/6000 [00:42<00:15, 120.89it/s]\u001b[A\n"," 68%|██████▊   | 4079/6000 [00:42<00:16, 118.52it/s]\u001b[A\n"," 68%|██████▊   | 4092/6000 [00:42<00:15, 119.49it/s]\u001b[A\n"," 68%|██████▊   | 4105/6000 [00:42<00:15, 121.87it/s]\u001b[A\n"," 69%|██████▊   | 4118/6000 [00:42<00:15, 120.17it/s]\u001b[A\n"," 69%|██████▉   | 4131/6000 [00:42<00:15, 120.99it/s]\u001b[A\n"," 69%|██████▉   | 4144/6000 [00:42<00:15, 119.57it/s]\u001b[A\n"," 69%|██████▉   | 4156/6000 [00:42<00:15, 118.45it/s]\u001b[A\n"," 69%|██████▉   | 4169/6000 [00:42<00:15, 119.12it/s]\u001b[A\n"," 70%|██████▉   | 4181/6000 [00:43<00:15, 117.79it/s]\u001b[A\n"," 70%|██████▉   | 4193/6000 [00:43<00:15, 115.86it/s]\u001b[A\n"," 70%|███████   | 4205/6000 [00:43<00:15, 116.14it/s]\u001b[A\n"," 70%|███████   | 4217/6000 [00:43<00:15, 116.67it/s]\u001b[A\n"," 70%|███████   | 4229/6000 [00:43<00:15, 116.71it/s]\u001b[A\n"," 71%|███████   | 4241/6000 [00:43<00:15, 115.74it/s]\u001b[A\n"," 71%|███████   | 4254/6000 [00:43<00:14, 118.60it/s]\u001b[A\n"," 71%|███████   | 4267/6000 [00:43<00:14, 119.58it/s]\u001b[A\n"," 71%|███████▏  | 4279/6000 [00:43<00:14, 118.43it/s]\u001b[A\n"," 72%|███████▏  | 4291/6000 [00:43<00:14, 115.95it/s]\u001b[A\n"," 72%|███████▏  | 4303/6000 [00:44<00:14, 116.20it/s]\u001b[A\n"," 72%|███████▏  | 4315/6000 [00:44<00:14, 116.72it/s]\u001b[A\n"," 72%|███████▏  | 4328/6000 [00:44<00:14, 117.94it/s]\u001b[A\n"," 72%|███████▏  | 4340/6000 [00:44<00:14, 117.27it/s]\u001b[A\n"," 73%|███████▎  | 4352/6000 [00:44<00:14, 116.47it/s]\u001b[A\n"," 73%|███████▎  | 4364/6000 [00:44<00:14, 116.24it/s]\u001b[A\n"," 73%|███████▎  | 4376/6000 [00:44<00:13, 116.07it/s]\u001b[A\n"," 73%|███████▎  | 4388/6000 [00:44<00:13, 116.29it/s]\u001b[A\n"," 73%|███████▎  | 4401/6000 [00:44<00:13, 117.99it/s]\u001b[A\n"," 74%|███████▎  | 4413/6000 [00:45<00:13, 117.98it/s]\u001b[A\n"," 74%|███████▍  | 4426/6000 [00:45<00:13, 119.84it/s]\u001b[A\n"," 74%|███████▍  | 4438/6000 [00:45<00:13, 118.94it/s]\u001b[A\n"," 74%|███████▍  | 4450/6000 [00:45<00:13, 118.31it/s]\u001b[A\n"," 74%|███████▍  | 4462/6000 [00:45<00:13, 118.20it/s]\u001b[A\n"," 75%|███████▍  | 4475/6000 [00:45<00:12, 120.36it/s]\u001b[A\n"," 75%|███████▍  | 4488/6000 [00:45<00:12, 120.46it/s]\u001b[A\n"," 75%|███████▌  | 4501/6000 [00:45<00:12, 118.18it/s]\u001b[A\n"," 75%|███████▌  | 4513/6000 [00:45<00:12, 118.12it/s]\u001b[A\n"," 75%|███████▌  | 4526/6000 [00:45<00:12, 118.90it/s]\u001b[A\n"," 76%|███████▌  | 4538/6000 [00:46<00:12, 118.97it/s]\u001b[A\n"," 76%|███████▌  | 4551/6000 [00:46<00:12, 120.18it/s]\u001b[A\n"," 76%|███████▌  | 4564/6000 [00:46<00:12, 119.32it/s]\u001b[A\n"," 76%|███████▋  | 4576/6000 [00:46<00:12, 117.59it/s]\u001b[A\n"," 76%|███████▋  | 4588/6000 [00:46<00:12, 115.07it/s]\u001b[A\n"," 77%|███████▋  | 4601/6000 [00:46<00:11, 116.76it/s]\u001b[A\n"," 77%|███████▋  | 4614/6000 [00:46<00:11, 119.61it/s]\u001b[A\n"," 77%|███████▋  | 4626/6000 [00:46<00:11, 117.78it/s]\u001b[A\n"," 77%|███████▋  | 4638/6000 [00:46<00:11, 117.83it/s]\u001b[A\n"," 78%|███████▊  | 4650/6000 [00:47<00:11, 118.21it/s]\u001b[A\n"," 78%|███████▊  | 4662/6000 [00:47<00:11, 118.48it/s]\u001b[A\n"," 78%|███████▊  | 4675/6000 [00:47<00:11, 120.15it/s]\u001b[A\n"," 78%|███████▊  | 4688/6000 [00:47<00:10, 120.66it/s]\u001b[A\n"," 78%|███████▊  | 4701/6000 [00:47<00:10, 120.33it/s]\u001b[A\n"," 79%|███████▊  | 4714/6000 [00:47<00:10, 120.44it/s]\u001b[A\n"," 79%|███████▉  | 4727/6000 [00:47<00:10, 118.85it/s]\u001b[A\n"," 79%|███████▉  | 4739/6000 [00:47<00:10, 115.98it/s]\u001b[A\n"," 79%|███████▉  | 4751/6000 [00:47<00:10, 115.58it/s]\u001b[A\n"," 79%|███████▉  | 4763/6000 [00:47<00:10, 115.29it/s]\u001b[A\n"," 80%|███████▉  | 4775/6000 [00:48<00:10, 116.06it/s]\u001b[A\n"," 80%|███████▉  | 4787/6000 [00:48<00:10, 115.62it/s]\u001b[A\n"," 80%|███████▉  | 4799/6000 [00:48<00:10, 116.31it/s]\u001b[A\n"," 80%|████████  | 4811/6000 [00:48<00:10, 113.83it/s]\u001b[A\n"," 80%|████████  | 4823/6000 [00:48<00:10, 112.46it/s]\u001b[A\n"," 81%|████████  | 4835/6000 [00:48<00:10, 114.38it/s]\u001b[A\n"," 81%|████████  | 4848/6000 [00:48<00:09, 116.30it/s]\u001b[A\n"," 81%|████████  | 4860/6000 [00:48<00:09, 114.82it/s]\u001b[A\n"," 81%|████████  | 4873/6000 [00:48<00:09, 117.58it/s]\u001b[A\n"," 81%|████████▏ | 4886/6000 [00:49<00:09, 118.86it/s]\u001b[A\n"," 82%|████████▏ | 4898/6000 [00:49<00:09, 118.94it/s]\u001b[A\n"," 82%|████████▏ | 4911/6000 [00:49<00:09, 120.50it/s]\u001b[A\n"," 82%|████████▏ | 4924/6000 [00:49<00:08, 121.25it/s]\u001b[A\n"," 82%|████████▏ | 4937/6000 [00:49<00:08, 121.42it/s]\u001b[A\n"," 82%|████████▎ | 4950/6000 [00:49<00:08, 122.23it/s]\u001b[A\n"," 83%|████████▎ | 4963/6000 [00:49<00:08, 120.07it/s]\u001b[A\n"," 83%|████████▎ | 4976/6000 [00:49<00:08, 121.27it/s]\u001b[A\n"," 83%|████████▎ | 4989/6000 [00:49<00:08, 121.09it/s]\u001b[A\n"," 83%|████████▎ | 5002/6000 [00:50<00:08, 119.64it/s]\u001b[A\n"," 84%|████████▎ | 5015/6000 [00:50<00:08, 119.29it/s]\u001b[A\n"," 84%|████████▍ | 5027/6000 [00:50<00:08, 118.58it/s]\u001b[A\n"," 84%|████████▍ | 5039/6000 [00:50<00:08, 117.40it/s]\u001b[A\n"," 84%|████████▍ | 5051/6000 [00:50<00:08, 115.59it/s]\u001b[A\n"," 84%|████████▍ | 5063/6000 [00:50<00:08, 114.97it/s]\u001b[A\n"," 85%|████████▍ | 5075/6000 [00:50<00:08, 115.18it/s]\u001b[A\n"," 85%|████████▍ | 5087/6000 [00:50<00:07, 115.99it/s]\u001b[A\n"," 85%|████████▍ | 5099/6000 [00:50<00:07, 116.91it/s]\u001b[A\n"," 85%|████████▌ | 5111/6000 [00:50<00:07, 114.88it/s]\u001b[A\n"," 85%|████████▌ | 5123/6000 [00:51<00:07, 115.79it/s]\u001b[A\n"," 86%|████████▌ | 5135/6000 [00:51<00:07, 114.77it/s]\u001b[A\n"," 86%|████████▌ | 5147/6000 [00:51<00:07, 113.42it/s]\u001b[A\n"," 86%|████████▌ | 5159/6000 [00:51<00:07, 112.49it/s]\u001b[A\n"," 86%|████████▌ | 5171/6000 [00:51<00:07, 113.43it/s]\u001b[A\n"," 86%|████████▋ | 5183/6000 [00:51<00:07, 113.46it/s]\u001b[A\n"," 87%|████████▋ | 5195/6000 [00:51<00:07, 114.77it/s]\u001b[A\n"," 87%|████████▋ | 5207/6000 [00:51<00:06, 116.04it/s]\u001b[A\n"," 87%|████████▋ | 5219/6000 [00:51<00:06, 116.61it/s]\u001b[A\n"," 87%|████████▋ | 5231/6000 [00:51<00:06, 117.36it/s]\u001b[A\n"," 87%|████████▋ | 5243/6000 [00:52<00:06, 116.18it/s]\u001b[A\n"," 88%|████████▊ | 5255/6000 [00:52<00:06, 113.41it/s]\u001b[A\n"," 88%|████████▊ | 5267/6000 [00:52<00:06, 111.23it/s]\u001b[A\n"," 88%|████████▊ | 5279/6000 [00:52<00:06, 106.84it/s]\u001b[A\n"," 88%|████████▊ | 5291/6000 [00:52<00:06, 108.46it/s]\u001b[A\n"," 88%|████████▊ | 5303/6000 [00:52<00:06, 110.23it/s]\u001b[A\n"," 89%|████████▊ | 5315/6000 [00:52<00:06, 110.28it/s]\u001b[A\n"," 89%|████████▉ | 5327/6000 [00:52<00:06, 111.85it/s]\u001b[A\n"," 89%|████████▉ | 5339/6000 [00:52<00:05, 112.34it/s]\u001b[A\n"," 89%|████████▉ | 5351/6000 [00:53<00:05, 111.75it/s]\u001b[A\n"," 89%|████████▉ | 5363/6000 [00:53<00:05, 111.96it/s]\u001b[A\n"," 90%|████████▉ | 5376/6000 [00:53<00:05, 113.95it/s]\u001b[A\n"," 90%|████████▉ | 5388/6000 [00:53<00:05, 111.95it/s]\u001b[A\n"," 90%|█████████ | 5400/6000 [00:53<00:05, 110.27it/s]\u001b[A\n"," 90%|█████████ | 5412/6000 [00:53<00:05, 109.41it/s]\u001b[A\n"," 90%|█████████ | 5423/6000 [00:53<00:05, 109.35it/s]\u001b[A\n"," 91%|█████████ | 5435/6000 [00:53<00:05, 110.27it/s]\u001b[A\n"," 91%|█████████ | 5447/6000 [00:53<00:04, 110.92it/s]\u001b[A\n"," 91%|█████████ | 5460/6000 [00:54<00:04, 114.18it/s]\u001b[A\n"," 91%|█████████ | 5472/6000 [00:54<00:04, 114.95it/s]\u001b[A\n"," 91%|█████████▏| 5484/6000 [00:54<00:04, 114.52it/s]\u001b[A\n"," 92%|█████████▏| 5496/6000 [00:54<00:04, 114.87it/s]\u001b[A\n"," 92%|█████████▏| 5508/6000 [00:54<00:04, 116.11it/s]\u001b[A\n"," 92%|█████████▏| 5520/6000 [00:54<00:04, 114.33it/s]\u001b[A\n"," 92%|█████████▏| 5532/6000 [00:54<00:04, 115.73it/s]\u001b[A\n"," 92%|█████████▏| 5544/6000 [00:54<00:03, 115.39it/s]\u001b[A\n"," 93%|█████████▎| 5556/6000 [00:54<00:03, 114.82it/s]\u001b[A\n"," 93%|█████████▎| 5568/6000 [00:54<00:03, 116.08it/s]\u001b[A\n"," 93%|█████████▎| 5580/6000 [00:55<00:03, 115.30it/s]\u001b[A\n"," 93%|█████████▎| 5592/6000 [00:55<00:03, 116.42it/s]\u001b[A\n"," 93%|█████████▎| 5604/6000 [00:55<00:03, 117.22it/s]\u001b[A\n"," 94%|█████████▎| 5616/6000 [00:55<00:03, 117.44it/s]\u001b[A\n"," 94%|█████████▍| 5628/6000 [00:55<00:03, 117.94it/s]\u001b[A\n"," 94%|█████████▍| 5641/6000 [00:55<00:03, 119.14it/s]\u001b[A\n"," 94%|█████████▍| 5653/6000 [00:55<00:02, 117.43it/s]\u001b[A\n"," 94%|█████████▍| 5665/6000 [00:55<00:02, 116.91it/s]\u001b[A\n"," 95%|█████████▍| 5677/6000 [00:55<00:02, 117.22it/s]\u001b[A\n"," 95%|█████████▍| 5690/6000 [00:56<00:02, 118.29it/s]\u001b[A\n"," 95%|█████████▌| 5702/6000 [00:56<00:02, 117.18it/s]\u001b[A\n"," 95%|█████████▌| 5714/6000 [00:56<00:02, 116.74it/s]\u001b[A\n"," 95%|█████████▌| 5726/6000 [00:56<00:02, 116.76it/s]\u001b[A\n"," 96%|█████████▌| 5739/6000 [00:56<00:02, 118.65it/s]\u001b[A\n"," 96%|█████████▌| 5751/6000 [00:56<00:02, 118.79it/s]\u001b[A\n"," 96%|█████████▌| 5763/6000 [00:56<00:02, 117.18it/s]\u001b[A\n"," 96%|█████████▋| 5775/6000 [00:56<00:01, 117.41it/s]\u001b[A\n"," 96%|█████████▋| 5788/6000 [00:56<00:01, 118.42it/s]\u001b[A\n"," 97%|█████████▋| 5801/6000 [00:56<00:01, 118.45it/s]\u001b[A\n"," 97%|█████████▋| 5813/6000 [00:57<00:01, 118.30it/s]\u001b[A\n"," 97%|█████████▋| 5825/6000 [00:57<00:01, 117.87it/s]\u001b[A\n"," 97%|█████████▋| 5837/6000 [00:57<00:01, 115.88it/s]\u001b[A\n"," 97%|█████████▋| 5849/6000 [00:57<00:01, 115.83it/s]\u001b[A\n"," 98%|█████████▊| 5861/6000 [00:57<00:01, 116.45it/s]\u001b[A\n"," 98%|█████████▊| 5873/6000 [00:57<00:01, 114.25it/s]\u001b[A\n"," 98%|█████████▊| 5885/6000 [00:57<00:00, 115.33it/s]\u001b[A\n"," 98%|█████████▊| 5898/6000 [00:57<00:00, 117.31it/s]\u001b[A\n"," 98%|█████████▊| 5910/6000 [00:57<00:00, 116.50it/s]\u001b[A\n"," 99%|█████████▊| 5922/6000 [00:58<00:00, 116.26it/s]\u001b[A\n"," 99%|█████████▉| 5935/6000 [00:58<00:00, 118.29it/s]\u001b[A\n"," 99%|█████████▉| 5947/6000 [00:58<00:00, 113.93it/s]\u001b[A\n"," 99%|█████████▉| 5959/6000 [00:58<00:00, 112.86it/s]\u001b[A\n","100%|█████████▉| 5971/6000 [00:58<00:00, 112.11it/s]\u001b[A\n","100%|█████████▉| 5983/6000 [00:58<00:00, 110.68it/s]\u001b[A\n","100%|██████████| 6000/6000 [00:58<00:00, 102.20it/s]\n"]}]}]}